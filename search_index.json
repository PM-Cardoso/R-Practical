[["index.html", "R-Practical 1 Opening", " R-Practical Pedro Cardoso 1 Opening This file should contain written practicals for VIMC practicals. "],["programming-practical---p1.html", "2 Programming Practical - P1 2.1 Getting started 2.2 Predicting the behaviour of for loops 2.3 Writing a for loop 2.4 Writing a while loop 2.5 Using loops to do more complex calculations 2.6 Writing simple functions 2.7 Predicting the behaviour of a function 2.8 Return values from functions 2.9 Additional tasks for those with programming experience 2.10 Appendix: relating sticker collecting to the harmonic numbers", " 2 Programming Practical - P1 2.1 Getting started Start a new RStudio session, set your current working directory, open a new R Script and save this (blank) script to a file with an appropriate name. Use this script file to edit and keep copies of the code you write in this practical: you may find it useful be able to look back later. 2.2 Predicting the behaviour of for loops We will use R to generate some data in the form of vectors and use these data to form some looping constructs. Predicting what code will do and testing your predictions by running the code should help cement your understanding of for loops. Type the following code into your script. Remember that code after # is a comment and will be ignored by the R interpreter (you do not have to type these comments in for yourself, but you might want to if you think you will need to refer to them later). Task a &lt;- 1:10 # Assign the integers 1 to 10 in order to the vector a print(a) # Print a to the screen ## [1] 1 2 3 4 5 6 7 8 9 10 b &lt;- sample(a) # Set b to be a with elements shuffled into random order print(b) # Print b to the screen ## [1] 1 10 3 2 4 7 6 5 9 8 Run the code a few times either by sourcing it or by highlighting it in the script then pressing the run button in RStudio. You should note that: The first line creates a new vector a with the elements 1 to 10 in order The second line prints the vector a to the screen The third line uses a built-in R function to sample (all of) the elements of the vector a without replacement, i.e., to permute the elements into a different order, and assigns the result to the vector b The fourth line prints the vector b 2.2.1 Example one Write down what you expect the output of the following code to be before verifying your answer by entering the code into your script and running it. Task for (i in 1:10) { print(paste(i, a[i])) } Show: Solution Solution Prints numbers in order. ## [1] &quot;1 1&quot; ## [1] &quot;2 2&quot; ## [1] &quot;3 3&quot; ## [1] &quot;4 4&quot; ## [1] &quot;5 5&quot; ## [1] &quot;6 6&quot; ## [1] &quot;7 7&quot; ## [1] &quot;8 8&quot; ## [1] &quot;9 9&quot; ## [1] &quot;10 10&quot; 2.2.2 Example two Write down what you expect the output of the following code to be before verifying your answer by entering the code into your script and running it. Task for (i in 1:10) { print(paste(i, b[i])) } Show: Solution Solution Prints numbers in reverse order according to whatever is in sample b. ## [1] &quot;1 1&quot; ## [1] &quot;2 10&quot; ## [1] &quot;3 3&quot; ## [1] &quot;4 2&quot; ## [1] &quot;5 4&quot; ## [1] &quot;6 7&quot; ## [1] &quot;7 6&quot; ## [1] &quot;8 5&quot; ## [1] &quot;9 9&quot; ## [1] &quot;10 8&quot; 2.2.3 Example three Write down what you expect the output of the following code to be before verifying your answer by entering the code into your script and running it. Task for (i in a) { print(paste(i, b[i])) } Show: Solution Solution Prints numbers in forward order according to whatever is in sample b. ## [1] &quot;1 1&quot; ## [1] &quot;2 10&quot; ## [1] &quot;3 3&quot; ## [1] &quot;4 2&quot; ## [1] &quot;5 4&quot; ## [1] &quot;6 7&quot; ## [1] &quot;7 6&quot; ## [1] &quot;8 5&quot; ## [1] &quot;9 9&quot; ## [1] &quot;10 8&quot; 2.2.4 Example four Write down what you expect the output of the following code to be before verifying your answer by entering the code into your script and running it. Task for (i in sample(a)) { print(paste(i, b[i])) } Show: Solution Solution Prints numbers in random order from whatever is in sample b. ## [1] &quot;7 6&quot; ## [1] &quot;8 5&quot; ## [1] &quot;6 7&quot; ## [1] &quot;5 4&quot; ## [1] &quot;4 2&quot; ## [1] &quot;10 8&quot; ## [1] &quot;1 1&quot; ## [1] &quot;9 9&quot; ## [1] &quot;3 3&quot; ## [1] &quot;2 10&quot; 2.3 Writing a for loop Adapt the code from the lecture to use a for loop to do the following: Task Print the 12 times table, i.e. produce output of the form 1 times 12 is 12 2 times 12 is 24  12 times 12 is 144 Show: Solution Solution for (i in 1:12) { result &lt;- i * 12 print(paste(i, &quot;times 12 is&quot;, result)) } ## [1] &quot;1 times 12 is 12&quot; ## [1] &quot;2 times 12 is 24&quot; ## [1] &quot;3 times 12 is 36&quot; ## [1] &quot;4 times 12 is 48&quot; ## [1] &quot;5 times 12 is 60&quot; ## [1] &quot;6 times 12 is 72&quot; ## [1] &quot;7 times 12 is 84&quot; ## [1] &quot;8 times 12 is 96&quot; ## [1] &quot;9 times 12 is 108&quot; ## [1] &quot;10 times 12 is 120&quot; ## [1] &quot;11 times 12 is 132&quot; ## [1] &quot;12 times 12 is 144&quot; 2.4 Writing a while loop Repeat the previous task using a while loop. Task Print the 12 times table, i.e. produce output of the form 1 times 12 is 12 2 times 12 is 24  12 times 12 is 144 Show: Solution Solution i &lt;- 1 while(i &lt;= 12) { result &lt;- i * 12 print(paste(i, &quot;times 12 is&quot;, result)) i &lt;- i + 1 } ## [1] &quot;1 times 12 is 12&quot; ## [1] &quot;2 times 12 is 24&quot; ## [1] &quot;3 times 12 is 36&quot; ## [1] &quot;4 times 12 is 48&quot; ## [1] &quot;5 times 12 is 60&quot; ## [1] &quot;6 times 12 is 72&quot; ## [1] &quot;7 times 12 is 84&quot; ## [1] &quot;8 times 12 is 96&quot; ## [1] &quot;9 times 12 is 108&quot; ## [1] &quot;10 times 12 is 120&quot; ## [1] &quot;11 times 12 is 132&quot; ## [1] &quot;12 times 12 is 144&quot; 2.5 Using loops to do more complex calculations 2.5.1 A loop for which the number of iterations is known in advance Enter the following code to your script and check you understand the output. p &lt;- 4 print(2^p) p &lt;- 5 print(p^3) Now write code to answer the following question. Task What is the sum of the first 15 powers of 2 (i.e. 21 + 22 +  215)? Show: Solution Solution runningSum &lt;- 0 for(i in 1:15) { runningSum &lt;- runningSum + 2^i } print(paste(&quot;Sum is&quot;,runningSum)) ## [1] &quot;Sum is 65534&quot; Note you can do the above more efficiently, e.g. sum(2^(1:15)) 2.5.2 A loop for which the number of iterations is not know in advance Write code to answer the following question. Task What is the smallest power of 2 that is greater than 1000000? Show: Solution Solution thisIDX &lt;- 1 thisPower &lt;- 2^thisIDX while(thisPower&lt;1000000) { thisIDX &lt;- thisIDX + 1 thisPower &lt;- 2^thisIDX } print(paste(&quot;2^&quot;,thisIDX,&quot;=&quot;,thisPower,sep=&quot;&quot;)) ## [1] &quot;2^20=1048576&quot; 2.6 Writing simple functions Type the following code into your script and run it. Before you run it, think about whether you expect any output. sayHello &lt;- function() { print(&quot;Hello&quot;) } Type the following at the command prompt, but before you do so, predict what will happen. sayHello() Now type the following code into your script. sayHelloWithArg &lt;- function(whoTo) { print(paste(&quot;Hello&quot;, whoTo)) } Task Now type each of the following instructions into the command prompt. Before typing each line, predict what will happen. sayHelloWithArg() sayHelloWithArg sayHelloWithArg(&quot;nik&quot;) sayHello(&quot;nik&quot;) sayHelloWithArg(nik) sayHelloWithArg(whoto=&quot;nik&quot;) sayHelloWithArg(whoTo=&quot;nik&quot;) Show: Solution Solution sayHelloWithArg() # error, since not passed argument sayHelloWithArg # echos function&#39;s code to screen (happens in general if type function&#39;s name without brackets) sayHelloWithArg(&quot;nik&quot;) # works as expected sayHello(&quot;nik&quot;) # error, since too many arguments for the simpler function sayHelloWithArg(nik) # error, since variable nik is not defined sayHelloWithArg(whoto=&quot;nik&quot;) # error, since argument names are case sensitive sayHelloWithArg(whoTo=&quot;nik&quot;) # works as expected 2.7 Predicting the behaviour of a function For now, do not type in the following code, but instead just read it. cat &lt;- 2 canary &lt;- 4 buster &lt;- function(cat, canary) { cat &lt;- cat*2 canary &lt;- canary*3 return(cat + canary) } ghost = buster(canary, -cat) Task Think carefully about what you expect the values of cat, canary and ghost to be after this code is executed and write down your answer. Now execute the code and test your prediction. Show: Solution Solution The line ghost = buster(canary, -cat) sets ghost to be 2, but leaves cat and canary unchanged. 2.8 Return values from functions The code in the box below does the following. Defines a function mySum that returns the sum of a vector Creates a vector x containing 100 uniform random numbers between 0 and 1 Finds their sum using mySum Finds their sum using the built-in R function sum Prints out a comparison of the two values Type the code into your script and make sure you are happy with how it works. mySum &lt;- function(toSum) { # my function to sum elements of a vector retVal &lt;- 0 for(i in 1:length(toSum)) { retVal &lt;- retVal + toSum[i] } return(retVal) } a &lt;- runif(100) # generate 100 uniform random numbers myWay &lt;- mySum(a) # calculate sum using our mySum() function rWay &lt;- sum(a) # calculate sum using built-in sum() function print(paste(&quot;My answer&quot;, myWay, &quot;... R answer&quot;, rWay)) Task Use the function mySum as a base to write a new function mySumAndMean. The new function should calculate the sum and the mean of whatever vector it receives as an input. It should return a vector: the first element of the output vector should be the sum, the second the mean. You might find the built-in function mean helpful to test your code. Show: Solution Solution mySumAndMean &lt;- function(toSum) { # my function to sum elements of a vector and return that and the mean retVal &lt;- 0 for(i in 1:length(toSum)) { retVal &lt;- retVal + toSum[i] } meanVal &lt;- retVal/length(toSum) # find the mean, too return(c(retVal,meanVal)) # return two values by wrapping up into vector } 2.9 Additional tasks for those with programming experience The following tasks are mainly aimed at those of you who already have experience in programming. 2.9.1 Extended example one: Monte Carlo simulation When I was younger, I sometimes collected Panini football stickers. The basic idea is very simple: you collect numbered stickers to stick into an album, continuing to buy stickers until you have the full set. The problem is that you dont know which stickers you are going to get until you open a packet. Each packet contains a random sample of the available stickers, and packets are sealed at the time of purchase. This means that as you get closer and closer to completing the album, it becomes more and more unlikely that any new packet will contain any stickers you need for your album. At the time, it was a matter of significant regret for me that I never did complete the album for the Mexico 86 World Cup. That album required a total of 427 stickers. Now, better trained in mathematics and computing, I wondered how many stickers I might have had to buy to be successful in my quest to complete the album. We are going to write a simple Monte Carlo simulation to understand this. Monte Carlo simulations rely on random sampling to obtain an estimate of a random variable. You will be writing Monte Carlo simulations of stochastic epidemic models later in the course, so this is good practice. We remove some of the complexity by considering a slightly simpler situation, in which stickers come in packets of one. Note the built-in R function sample that was introduced earlier can in fact be called in other ways. Type the following into the command line and run it a few times sample(1:200,3) You should see that  when it is called with two arguments  the built-in R function sample no longer merely shuffles the whole of the vector that is its first argument, but instead randomly samples the number of elements given by the second argument from that vector (again without replacement). The above corresponds to randomly choosing a three numbers from the list of integers between 1 and 200. Furthermore, note the behaviour of the built-in R function rep by typing the following in at the command line rep(TRUE,4) You should see that this line of code creates a vector of length 4 where all four elements are the logical value TRUE. One way of writing a simulation of a single attempt to collect the entire album would be something like the code that follows. However, this code contains a deliberate mistake. Task Enter the code into your script, find the mistake, fix it, and run the code. totStickers &lt;- 427 gotSticker &lt;- rep(FALSE,totStickers) numLeft &lt;- totStickers numStickersBought &lt;- 0 while(numLeft &gt; 0) { numStickersBought &lt;- numStickersBought + 1 thisSticker &lt;- sample(1:totStickers,1) if(gotSticker[thisSticker]==TRUE) { gotSticker[thisSticker] &lt;- TRUE numLeft &lt;- numLeft - 1 print(paste(&#39;got&#39;, thisSticker, &#39;still need&#39;,numLeft)) } } print(paste(&#39;bought&#39;, numStickersBought)) Show: Solution Solution There is a mistake in the code on line 8. The condition in the if statement should be checking for FALSE, not TRUE. Each time you run the fixed version of the code should lead to a different result. You can get an estimate of the average number of stickers that is required to complete the album by running the code lots of times, and averaging the result. Task Add a for loop around the code above to run 100 simulations of collecting an entire album, storing the number of stickers required in each simulation in a vector. Apply the function built-in R function mean to the vector to estimate the average number of stickers that must be bought to complete the album. Show: Solution Solution myEstimates &lt;- numeric(0) numRuns &lt;- 100 for(i in 1:numRuns) { totStickers &lt;- 427 gotSticker &lt;- rep(FALSE,totStickers) numLeft &lt;- totStickers numStickersBought &lt;- 0 while(numLeft &gt; 0) { numStickersBought &lt;- numStickersBought + 1 thisSticker &lt;- sample(1:totStickers,1) if(gotSticker[thisSticker]==FALSE) { # this is the line which had an error (fixed here) gotSticker[thisSticker] &lt;- TRUE numLeft &lt;- numLeft - 1 # commented out the following print statement to save time # print(paste(&#39;got&#39;, thisSticker, &#39;still need&#39;,numLeft)) } } print(paste(&quot;Run&quot;,i,&quot;bought&quot;, numStickersBought)) myEstimates[i] &lt;- numStickersBought } ## [1] &quot;Run 1 bought 2892&quot; ## [1] &quot;Run 2 bought 3987&quot; ## [1] &quot;Run 3 bought 2626&quot; ## [1] &quot;Run 4 bought 3780&quot; ## [1] &quot;Run 5 bought 2280&quot; ## [1] &quot;Run 6 bought 3211&quot; ## [1] &quot;Run 7 bought 3078&quot; ## [1] &quot;Run 8 bought 2661&quot; ## [1] &quot;Run 9 bought 3480&quot; ## [1] &quot;Run 10 bought 2651&quot; ## [1] &quot;Run 11 bought 2900&quot; ## [1] &quot;Run 12 bought 3301&quot; ## [1] &quot;Run 13 bought 2793&quot; ## [1] &quot;Run 14 bought 2242&quot; ## [1] &quot;Run 15 bought 2798&quot; ## [1] &quot;Run 16 bought 2489&quot; ## [1] &quot;Run 17 bought 3094&quot; ## [1] &quot;Run 18 bought 3684&quot; ## [1] &quot;Run 19 bought 2588&quot; ## [1] &quot;Run 20 bought 2947&quot; ## [1] &quot;Run 21 bought 2918&quot; ## [1] &quot;Run 22 bought 2581&quot; ## [1] &quot;Run 23 bought 4000&quot; ## [1] &quot;Run 24 bought 3198&quot; ## [1] &quot;Run 25 bought 3811&quot; ## [1] &quot;Run 26 bought 2935&quot; ## [1] &quot;Run 27 bought 2673&quot; ## [1] &quot;Run 28 bought 3214&quot; ## [1] &quot;Run 29 bought 2566&quot; ## [1] &quot;Run 30 bought 2471&quot; ## [1] &quot;Run 31 bought 2457&quot; ## [1] &quot;Run 32 bought 2582&quot; ## [1] &quot;Run 33 bought 2490&quot; ## [1] &quot;Run 34 bought 2443&quot; ## [1] &quot;Run 35 bought 2642&quot; ## [1] &quot;Run 36 bought 3243&quot; ## [1] &quot;Run 37 bought 3039&quot; ## [1] &quot;Run 38 bought 2886&quot; ## [1] &quot;Run 39 bought 3610&quot; ## [1] &quot;Run 40 bought 3230&quot; ## [1] &quot;Run 41 bought 2301&quot; ## [1] &quot;Run 42 bought 2173&quot; ## [1] &quot;Run 43 bought 2931&quot; ## [1] &quot;Run 44 bought 1967&quot; ## [1] &quot;Run 45 bought 2369&quot; ## [1] &quot;Run 46 bought 2550&quot; ## [1] &quot;Run 47 bought 2491&quot; ## [1] &quot;Run 48 bought 3804&quot; ## [1] &quot;Run 49 bought 2308&quot; ## [1] &quot;Run 50 bought 3164&quot; ## [1] &quot;Run 51 bought 3037&quot; ## [1] &quot;Run 52 bought 2091&quot; ## [1] &quot;Run 53 bought 2185&quot; ## [1] &quot;Run 54 bought 2114&quot; ## [1] &quot;Run 55 bought 2097&quot; ## [1] &quot;Run 56 bought 2524&quot; ## [1] &quot;Run 57 bought 2284&quot; ## [1] &quot;Run 58 bought 3062&quot; ## [1] &quot;Run 59 bought 2274&quot; ## [1] &quot;Run 60 bought 2605&quot; ## [1] &quot;Run 61 bought 4299&quot; ## [1] &quot;Run 62 bought 2429&quot; ## [1] &quot;Run 63 bought 3046&quot; ## [1] &quot;Run 64 bought 2396&quot; ## [1] &quot;Run 65 bought 2734&quot; ## [1] &quot;Run 66 bought 3590&quot; ## [1] &quot;Run 67 bought 2084&quot; ## [1] &quot;Run 68 bought 3595&quot; ## [1] &quot;Run 69 bought 2993&quot; ## [1] &quot;Run 70 bought 2261&quot; ## [1] &quot;Run 71 bought 3571&quot; ## [1] &quot;Run 72 bought 2664&quot; ## [1] &quot;Run 73 bought 2640&quot; ## [1] &quot;Run 74 bought 2755&quot; ## [1] &quot;Run 75 bought 2367&quot; ## [1] &quot;Run 76 bought 2228&quot; ## [1] &quot;Run 77 bought 2338&quot; ## [1] &quot;Run 78 bought 2522&quot; ## [1] &quot;Run 79 bought 2204&quot; ## [1] &quot;Run 80 bought 2745&quot; ## [1] &quot;Run 81 bought 2896&quot; ## [1] &quot;Run 82 bought 2397&quot; ## [1] &quot;Run 83 bought 2800&quot; ## [1] &quot;Run 84 bought 2173&quot; ## [1] &quot;Run 85 bought 3001&quot; ## [1] &quot;Run 86 bought 2616&quot; ## [1] &quot;Run 87 bought 2746&quot; ## [1] &quot;Run 88 bought 2822&quot; ## [1] &quot;Run 89 bought 3164&quot; ## [1] &quot;Run 90 bought 3090&quot; ## [1] &quot;Run 91 bought 2343&quot; ## [1] &quot;Run 92 bought 3609&quot; ## [1] &quot;Run 93 bought 4124&quot; ## [1] &quot;Run 94 bought 1883&quot; ## [1] &quot;Run 95 bought 2332&quot; ## [1] &quot;Run 96 bought 3085&quot; ## [1] &quot;Run 97 bought 3552&quot; ## [1] &quot;Run 98 bought 3173&quot; ## [1] &quot;Run 99 bought 2682&quot; ## [1] &quot;Run 100 bought 3427&quot; hist(myEstimates) mean(myEstimates) ## [1] 2821.78 The expected value (i.e. mean) number of stickers that is required to complete an album of 427 stickers is in fact \\(427 \\times H_{427}\\), where \\(H_{427}\\) is the 427\\(^{th}\\) Harmonic number (the mathematics of this is explained in the appendix). It turns out that \\(H_{427} \\approx 6.635\\), and so the estimated mean is about 2,800 or so. This is a lot of stickers! The requisite number would be reduced in practice, because you are able swap duplicate stickers with your friends, as well as send off to Panini on a special form for a maximum of 50 stickers towards the end, but nevertheless, this simulation has explained my disappointment as a 9-year-old child! 2.9.2 Extended example two: the logistic map The logistic map is a simple population model in discrete-time, and relates the size of a population in generation n+1 to its size in generation n. It is one of the simplest models that shows chaotic behaviour. One way of writing the logistic map is via the following recurrence \\[x_{n+1} = rx_{n}(1-x_{n}) \\] where \\(x_{n}\\) is the population size in year \\(n\\), measured as a fraction of the carrying capacity, and \\(r\\) is the intrinsic rate of increase (a growth rate). Task What do you think is the purpose of the following function? logisticValues &lt;- function(r,N,x0) { retVal &lt;- numeric(N) thisX &lt;- x0 for(i in 1:N) { thisX &lt;- r * thisX * (1 - thisX) retVal[i] &lt;- thisX } return(retVal) } Show: Solution Solution The given function returns \\(N\\) iterates of the logistic map starting at \\(x_0\\) (note, does not store initial value) Include the function logisticValues in your script. Also include the following code that uses the function to plot a graph. r &lt;- 1.5 # growth rate N &lt;- 100 # max generation x0 &lt;- 0.01 # initial value xn &lt;- logisticValues(r,N,x0) # next N starting at x0 plot(0:N,c(x0,xn),type=&#39;o&#39;,xlab=&#39;n&#39;,ylab=&#39;x_n&#39;) # plot Task Try running the code for different values of r. What does the graph show you? Show: Solution Solution This code shows the behaviour of the logistic map with \\(r = 1.5\\) for 100 generations. Note the slightly clunky way of making sure the initial point is returned. Good values of \\(r\\) to try would be \\(r = 0.5\\), \\(r = 1.5\\), \\(r = 2.5\\), \\(r = 3.1\\), \\(r = 3.5\\), \\(r = 3.7\\). You should also test the behaviour for different initial conditions: suitable values would be \\(x_{0} = 0.01\\) and \\(x_{0} = 0.1\\). Note: As you change the value of \\(r\\) in the above code the system goes from having a single equilibrium in the long term, to oscillating between a pair of values, to oscillating between four values. By \\(r = 3.7\\), the trajectory depends very sensitively to the initial value \\(x0\\) (as is demonstrated by the below code). This is called deterministic chaos. You should note that in the case \\(r = 3.7\\), the pattern of successive population sizes appears to be fairly random. It also depends on the initial value of \\(x0\\). The following code confirms this (note the first five lines are almost exactly as they were before): include this code in your script to check this. r &lt;- 3.7 # growth rate N &lt;- 100 # max generation x0 &lt;- 0.01 # initial value xn &lt;- logisticValues(r,N,x0) # next N starting at x0 plot(0:N,c(x0,xn),type=&#39;o&#39;,xlab=&#39;n&#39;,ylab=&#39;x_n&#39;) # plot x0 &lt;- 0.01001 # different initial value xn &lt;- logisticValues(r,N,x0) # next N starting there lines(0:N,c(x0,xn),type=&#39;o&#39;,col=&#39;red&#39;) # overlay on plot Notice that, despite starting very close together, the trajectories diverge over time. Sensitive dependence on initial conditions is characteristic of deterministic chaos. Task What do you think the following code does? Test your understanding by typing the function in and running it by issuing an appropriate call to the function at the command prompt. logisticValuesAfterBurnIn &lt;- function(r,N,M,x0) { retVal &lt;- numeric(M) thisX &lt;- x0 for(i in 1:N) { thisX &lt;- r * thisX * (1 - thisX) } for(i in 1:M) { thisX &lt;- r * thisX * (1 - thisX) retVal[i] &lt;- thisX } return(retVal) } Show: Solution Solution The given code performs \\(N\\) iterations of the logistic map starting from \\(x_0\\) but does not store them. It then calculates, stores and returns the following \\(M\\) iterations of the logistic map. Enter the following code and run it. rVals &lt;- seq(2.5,4,by=0.005) plot(c(2.5,4),c(0,1),type=&#39;n&#39;,xlab=&#39;r&#39;,ylab=&#39;orbit&#39;) for(r in rVals) { N &lt;- 1000 M &lt;- 100 xn &lt;- logisticValuesAfterBurnIn(r,N,M,0.01) points(rep(r,M),xn,pch=19,cex=0.01) } Task What happens? Do you understand what the plot  a so-called bifurcation diagram - is showing? Show: Solution Solution The key idea is that - for each value of \\(r\\) - it calculates \\(N=1000\\) iterations of the map, but totally ignores them. It then plots the values of the next 100 iterations (as the y-value on a graph which has the value of \\(r\\) as the x-ordinate). The idea behind all this is to let the system equilibriate - if it is ever going to do so. So from the bifurcation diagram you can see from this picture that there is/are: a single equilibrium for \\(2.5 &lt; r &lt; 3.0\\) two equilibria for \\(3.0 &lt; r &lt; 3.45\\) (-ish) four equilibria for \\(3.45\\)(-ish) \\(&lt; r &lt; 3.54\\) (-ish) by \\(r \\sim 3.57\\), we have chaos (as seen in plots for different starting conditions above) which here corresponds to a more or less random - although lying within certain bounds - set of 100 points plotted for each \\(r\\) This is explained well on the relevant wikipedia page A very readable introduction to deterministic chaos in biology is given by May (1976) (Simple mathematical models with very complicated dynamics. Nature 261: 459-467). That article is widely available online and comes highly recommended. 2.10 Appendix: relating sticker collecting to the harmonic numbers In the lecture I claimed that the example of collecting things is somehow related to the harmonic series. To see this does not require sophisticated mathematics. If there are a total of \\(N\\) stickers, of which you have already collected \\(M\\), the probability of a new sticker being one you need is \\(p_{M} = (N-M)/N\\). Assuming that successive purchases are independent, the number of stickers that must be bought to get a sticker you need when you already have \\(M\\) stickers in your album would follow a geometric probability distribution1. The expected number of stickers needed to get the \\(M^{th}\\) sticker is therefore \\(\\frac{1}{p_{M}}\\). The number of stickers that you must buy to complete the entire album is then just the expected number of stickers to get the first sticker in your album (i.e. when you have already got 0), plus the expected number required for your second sticker (i.e. when you have already got 1), , all the way up to the expected number to get the final sticker (i.e. when you have already got \\(N  1\\)). Therefore \\[\\text{Average number of stickers required} = \\frac{1}{p_{0}} + \\frac{1}{p_{1}} + \\text{... } +\\frac{1}{p_{N-1}}.\\] However (by the definition of \\(p_{M}\\)) this reduces to \\[\\begin{aligned} \\text{Average number of stickers required} &amp;= \\frac{N}{N} + \\frac{N}{N-1} + \\text{... } + \\frac{N}{1} \\\\ &amp;=N\\left(\\frac{N}{N} + \\frac{N}{N-1} + \\text{... } + \\frac{1}{1}\\right) \\\\ &amp;=N\\left(1 + \\text{... } + \\frac{1}{N-1} + \\frac{1}{N}\\right) \\\\ &amp;=N \\times H_{N}\\end{aligned} \\] where we reversed the order of the sum in going from line 2 to line 3, and have used the definition of the \\(N^{th}\\) harmonic number (see lecture slides) on line 4. You will be reminded of basic facts about probability distributions in a forthcoming lecture. Note the expected number is quite intuitive: as a simple example, consider rolling a dice until you get a six. The probability of getting a six is \\(\\frac{1}{6}\\): hopefully it seems reasonable that you should have to roll the dice an average of 6 times to get one. Thats all that is happening here, just written in symbols. "],["programming-practical---p2.html", "3 Programming Practical - P2 3.1 Random variables 3.2 Probability distributions 3.3 Using probability distributions in R - types 3.4 Example one - using probabilities for simulation 3.5 Example two - multiple choices 3.6 Exercise one - using R to model a die 3.7 Example three - using R to model a death process stochastically", " 3 Programming Practical - P2 3.1 Random variables A random variable can be thought of as the outcome of an experiment or a measurement i.e. something that can change each time you look at it. Examples: The number of heads you obtain when tossing a coin 10 times The number of emails you received in one hour The lifetime of a light-bulb The length of time an individual is infected for before recovering 3.2 Probability distributions A probability distribution will tell us the probability of a random variable taking a specific value. A Binomial Distribution will be able to give us the probability of obtaining \\(x\\) heads if we toss a coin 10 times (where \\(x\\) is any number). plot( x = 0:10, y = dbinom(0:10, 10, prob = 0.5), type = &quot;h&quot;, main = &quot;Binomial Probability \\nMass Function&quot;, xlab = &quot;Number of Heads&quot;, ylab = &quot;Probability&quot; ) This is an example of a discrete random variables, and so the distribution is called probability mass function. For continuous random variables we can only calculate the probability of being in an interval. A Normal Distribution will be able to gives us the probability of measurement errors, \\(x\\), being a certain size. plot( x = seq(-3, 3, length = 100), y = dnorm(seq(-3, 3, length = 100), mean = 0, sd = 1), type = &quot;l&quot;, main = &quot;Normal Probability \\nDensity Function&quot;, xlab = &quot;x&quot;, ylab = &quot;Probability Density&quot; ) This is an example of a continuous random variable, and so the distribution is called a probability density function. 3.3 Using probability distributions in R - types R has a large number of inbuilt probability distributions. Discrete distributions: Binomial Geometric Poisson Negative Binomial Hypergeometric Multinomial Continuous distributions: Normal Uniform T (T-test) F (F-test) Exponential Cauchy Beta Gamma Weibull Chi-squared Logistic R deals with them all in essentially the same way. Each inbuilt distribution in R is referred to by its abbreviation. Binomial: binom Poisson: pois Geometric: geom Normal: norm Uniform: unif Exponential: exp  Each inbuilt distribution in R has the same four generic functions associated with it. For a distribution with abbreviation dist: ddist(): density/mass function pdist(): cumulative density/mass function rdist(): random number generator function qdist(): quantile function 3.3.1 ddist() For a distribution type dist, the function ddist(x,...) returns: The probability \\(P(X=x)\\) (for discrete variables) or The probability density at \\(x\\) (for continuous variables) This can be used to calculate actual probabilities for discrete distributions. 3.3.1.1 dbinom() - discrete distribution Consider a Binomial Distribution with 10 trials, each with a probability of success of 0.5. The R function dbinom() describes the distribution of the probability of success. It takes the following arguments: \\(x\\): the number of successful trials size: the total number of trials prob: the probability of success dbinom(4, 10, 0.5) ## [1] 0.2050781 Or to plot the distribution itself. dbinom(0:10, 10, 0.5) ## [1] 0.0009765625 0.0097656250 0.0439453125 0.1171875000 0.2050781250 ## [6] 0.2460937500 0.2050781250 0.1171875000 0.0439453125 0.0097656250 ## [11] 0.0009765625 plot(0:10, dbinom(0:10, 10, 0.5), type = &quot;h&quot;, xlab = &quot;x&quot;, ylab = &quot;Probability Density&quot;) 3.3.1.2 dnorm() - continuous distribution Consider a Normal Distribution with mean 0 and standard deviation 1. The R function dnorm() gives the probability density at a given value of \\(x\\). It takes the following arguments: \\(x\\) mean sd: standard deviation The function dnorm(x, mean, sd) isnt useful in itself but is useful for plotting. With a continuous distribution we have to choose the \\(x\\) coordinates close enough together to approximate a smooth curve. x &lt;- seq(-4, 4, 0.01) plot(x, dnorm(x, 0, 1), type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = &quot;Probability Density&quot;) 3.3.2 pdist() For a distribution dist pdist(x, ...) returns the probability \\(P(X\\le x)\\) (which in a continuous function is an actual probability). 3.3.2.1 pnorm() - probability calculations If we want to find the probability of having a value below \\(-1\\) from a Normal Distribution we use pnorm(). pnorm(-1, 0, 1) ## [1] 0.1586553 If we want to find the probability of having a value between \\(-1\\) and \\(1.5\\) from a Normal Distribution we use pnorm(). pnorm(1.5, 0, 1) - pnorm(-1, 0, 1) ## [1] 0.7745375 3.3.3 rdist() For a distribution dist rdist() returns \\(n\\) random numbers drawn from that distribution. This function is of great use when trying to run stochastic models. The ability to draw random numbers from many different types of distribution is incredibly useful. 3.3.4 rnorm() - Normal Distribution Consider a Normal Distribution with mean and standard deviation. The R function rnorm() returns random numbers drawn from this distribution. It takes the following arguments: \\(n\\): number of random numbers to return mean sd: standard deviation norm.dat &lt;- rnorm(5, 0, 1) print(norm.dat) ## [1] 0.04816033 0.37308178 -0.33625093 -1.72855767 -0.55866863 Consider a Normal Distribution with mean 0 and variable 1. We will draw thousands of random numbers from this distribution and plot a histogram of them. This should then look like the theoretical Normal Distribution curve we produce with dnorm(). norm.dat &lt;- rnorm(50000, 0, 1) hist(norm.dat, prob = T, breaks = 50, xlab = &quot;x&quot;) x &lt;- seq(-4, 4, 0.01) lines(x, dnorm(x, 0, 1), col = &quot;red&quot;) 3.4 Example one - using probabilities for simulation Consider tossing a biased coin. There are two outcomes: Heads or Tails. \\(P(H) = \\frac{1}{3}\\) &amp; \\(P(T) = \\frac{2}{3}\\) How can we use probability distributions in R to simulate this? 3.4.1 Binomial Distribution Using the Binomial Distribution: 1 observation 1 trial Probability of success (H) is \\(\\frac{1}{3}\\) rbinom( 1, # number of observation 1, # number of binomial trials 1/3 # binomial probability of success ) ## [1] 1 rbinom() returns the number of successes from our 1 observation of tossing a biased coin once. 0: \\(\\frac{2}{3}\\) of the time 1: \\(\\frac{1}{3}\\) of the time We can check this by getting R to do many observations at once rbinom(15, 1, 1/3) ## [1] 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 3.4.2 Normal Distribution Using the Normal Distribution: Generate single number uniformly between 0 and 1. rand &lt;- runif( 1, # number of observation 0, # minimum 1 # maximum ) if (rand &lt; 1/3) { print(&quot;Heads&quot;) } else { print(&quot;Tails&quot;) } ## [1] &quot;Heads&quot; print(rand) # confirm by printing rand ## [1] 0.1177264 The shaded area corresponds to the probability of drawing Heads. 3.5 Example two - multiple choices Consider tossing a very thick biased coin. There are three outcomes: Heads, Tails, or edge. \\(P(H) = \\frac{3}{10}\\) \\(P(T) = \\frac{6}{10}\\) \\(P(E) = \\frac{1}{10}\\) Task How can we use probability distributions in R to simulate this? rand &lt;- runif(1, 0, 1) if (rand &lt; 3/10) { print(&quot;Heads&quot;) } else if (rand &lt; 9/10) { print(&quot;Tails&quot;) } else { print(&quot;Edge&quot;) } ## [1] &quot;Tails&quot; print(rand) # confirm by printing rand ## [1] 0.6496403 The blue shaded area corresponds to the probability of drawing Heads. The red shaded area corresponds to the probability of drawing Tails. The unshaded area corresponds to the probability of drawing Edge. 3.6 Exercise one - using R to model a die Consider a regular six-sided die. Task How could we simulate a single throw of a die? What are the possible outcomes? Use runif() Use (lots of) if else statements Show: Solution Solution rand &lt;- runif(1, 0, 1) if (rand &lt; 1/6) { print(1) } else if (rand &lt; 2/6) { print(2) } else if (rand &lt; 3/6) { print(3) } else if (rand &lt; 4/6) { print(4) } else if (rand &lt; 5/6) { print(5) } else { print(6) } ## [1] 2 3.7 Example three - using R to model a death process stochastically Consider a population of animals. Every year each animal has a fixed probability of dying (depressing I know!), independently of each other and how old they are. There are no new animals born. Task How could we model this? Need a program that keeps track of number of live animals At each time step, we need to roll a die to see if each live animal survives We update the numbers of live animals and keep going until everything dies! This is easy to do in R. nAlive &lt;- 100 # current number of live animals max.time &lt;- 200 # keep going for up to 200 steps nAliveStore &lt;- numeric(max.time+1) # vector to store how many were alive at each step nAliveStore[1] &lt;- nAliveStore # store how many were alive at the start ## Warning in nAliveStore[1] &lt;- nAliveStore: number of items to replace is not a ## multiple of replacement length for (iTim in 1:max.time) { # loop over all time steps pr &lt;- runif(nAlive, 0, 1) # get probability of dying for all currently live animals died &lt;- which(pr &lt; 0.05) # find out which animals have died nDead &lt;- length(died) # count up how many animals have died nAlive &lt;- nAlive - nDead # update the number of currently living animals nAliveStore[iTim + 1] &lt;- nAlive # store the number of living animals if (nAlive &lt;= 0) {break} # stop process when all animals are dead } plot(nAliveStore, type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Number alive&quot;) "],["dynamic-models-practical.html", "4 Dynamic models Practical 4.1 Introduction to deSolve", " 4 Dynamic models Practical 4.1 Introduction to deSolve 4.1.1 A bit of theory 4.1.1.1 Rationale: We have discussed the solutions of two differential equations using the fact that we were able to solve them analytically: \\[\\begin{aligned} \\frac{dN}{dt} &amp;= rN \\rightarrow N(t) = N_{0}e^{rt} \\\\ \\frac{dN}{dt} &amp;= rN\\left(1-\\frac{N}{K}\\right) \\rightarrow N(t) = \\frac{K}{1+\\frac{K-N_{0}}{N_{0}}e^{rt}}\\end{aligned}\\] However, most differential equations cannot be solved analytically. For example, with the predation model presented in Dynamic Models Lecture 2, we cannot obtain a mathematical formula giving the number of bacteria (prey) \\(N(t)\\) and amoeba (predators) \\(P(t)\\) as functions of the parameters and initial conditions. Instead we have to resort to numerical methods to obtain approximate values of \\(N(t)\\) and \\(P(t)\\) for any given set of parameter values. Since this is the case for the vast majority of differential equations of interest in science, such numerical methods have been developed for many years and the advent of computers have made them increasingly popular. Throughout this course we will be using a ready-made package in R called deSolve which can solve numerically virtually any system of ordinary differential equations (ODE). Note: in earlier versions of R, the package was called odesolve but worked in the same way. 4.1.1.2 A few notes on Eulers method Although we will be using deSolve as a black box, for your information here is a very basic introduction to how differential equation solvers work. The general principle was devised by Swiss mathematician Leonhard Euler (17071783). Among many other things, Euler also came up with the fascinating formula \\(e-i\\pi + 1 = 0\\), but thats another story. Here I illustrate the method with a single differential equation, but it can easily be extended to any number. Consider the logistic growth model with an arbitrary choice of parameter values: \\(dN/dt = 0.5N(1 - N/100)\\) and initial condition \\(N(0) = 1\\). The trick is then to discretise this equation, i.e. replace the infinitesimal variations of the differential equation by small discrete variations. Remember the formal definition of the derivative of a function: \\[\\frac{dN}{dt} = lim_{\\delta t\\rightarrow 0}\\frac{N(t+\\delta t)-N(t)}{\\delta t}\\] We then approximate the variations during a small time step \\(\\delta t\\) as follows: \\[ \\frac{N(t+\\delta t)-N(t)}{\\delta t} \\approx 0.5N(t)\\left( 1-\\frac{N(t)}{100}\\right)\\] So, if we know the value of \\(N(t)\\) at a given time \\(t\\), we can estimate the value at the next time point: \\[ N(t+\\delta t) \\approx N(t) + 0.5N(t)\\left( 1-\\frac{N(t)}{100} \\right) \\delta t\\] If you know the initial density, say \\(N(0) = 1\\), and you choose \\(\\delta t = 0.1\\), then you can iteratively calculate approximate values of \\(N(0.1) \\approx 1.0495\\), \\(N(0.2) \\approx 1.1014\\), etc. which eventually enables you to reconstruct the numerical solution \\(N(t)\\). The only problem is that \\(N(0.2)\\) is estimated based on an approximate value of \\(N(0.1)\\), and so on. So, as you iterate the process, you accumulate errors and you run the risk of seeing your numerical solution drift away from the true solution. Of course, the smaller \\(\\delta t\\), the more accurate the discretisation. See the graph below, where the solid curve is the true solution of the above differential equation (which, in this case, can be solved analytically) and the dots are two discretised solutions, using \\(\\delta t = 1\\) (discs) and \\(\\delta t = 0.5\\) (squares): So, thats basically what deSolve or any other software does to generate numerical solutions of differential equations. They actually use more sophisticated algorithms (but based on the same discretisation principle) in order to reduce the potential drift from the (unknown) true solution. Different algorithms will perform better on different classes of models, but they are not 100% accurate. One of the most popular class of algorithms for the kind of ODEs used in population dynamics was designed by German mathematicians Runge &amp; Kutta almost 100 years ago. 4.1.2 The return of the logistic model 4.1.2.1 Load the deSolve package But enough theory for now. Lets shut the bonnet and sit behind the wheel! Well start by re-visiting the logistic growth model to get you acquainted with deSolve. Launch RStudio. The first thing to do is check whether the deSolve package is already installed, as it does not come with the R base. In RStudio, click on the Packages tab in the bottom-right window to see the list of R packages installed on your computer (those with a checked box are not only installed on the computer but currently loaded in your R session so that you can use them). If deSolve is not in the list, click on Install Packages, this opens a window asking you which package you which to install: type deSolve and click on Install (you may be asked first to choose a Mirror: scroll down the list to UK, and choose London or Bristol). This will download the package from the internet, which means your computer needs to be connected. You only need to do that once on any computer. Note: Instead of using RStudios interface, you can type the following instructions in the console: library() will open a window with the list of installed packages. If deSolve is not there, close the window, return to the R console and type: install.packages('deSolve'). Once a package is installed on a computer, you still need to tell R to load the package into the memory every time you start a new session. In RStudio you can do that by checking the box of the package in the Packages window, but if you need to use the package in a script file, it is safer to type the command library(deSolve). Were now ready to start. Open a new script file for this practical code, write some comments (starting with a #) that will remind you whats in this file next time you open it, and type the first instruction: library(deSolve) REMEMBER TO SAVE YOUR FILE REGULARLY 4.1.2.2 Define the model First, we must define a function logistic_dyn() that returns the value of the derivative \\(dN/dt\\), in this case \\(rN(1 - N/K)\\), given the values of \\(r\\), \\(K\\) and \\(N\\). This function will be used by the differential equation solver to generate the numerical solution \\(N(t)\\), so we have to follow specific rules set by the authors of the deSolve package: The function that defined the derivative must take three arguments: time, a vector containing the values of the variables (in this case a single variable \\(N\\)) and a vector of parameter values (in this case two parameters \\(r\\) and \\(K\\)). The function must return a list containing the value(s) of the derivative(s). Note that the name of the function, logistic_dyn, is arbitrary; you may call it anything you like. Type the following code in your script file: #This function calculates dN/dt for the logistic model logistic_dyn &lt;- function(t, N, par){ #rename the parameters r &lt;- par[1] K &lt;- par[2] #calculate the derivative dN &lt;- r*N*(1-N/K) #last instruction: return a list return(list(dN)) } Note that before calculating the derivative, I defined a few local variables \\(r\\), \\(K\\) and \\(dN\\) (which only exist inside the function, so they do not appear in the Workspace) for convenience. A shorter version of the function, without local variables, would be: logistic_dyn &lt;- function(t, N, par) { list(par[1]*N*(1-N/par[2])) } but youll probably agree that it is less easy to read for a human being. The only case when you should favour the latter version is when speed (or memory) is an issue. Next, create three variables: a vector of parameter values (2 values in the order specified in logistic_dyn: \\(r\\) first and \\(K\\) second): logistic_par &lt;- c(1, 100) a vector of time steps at which we want values of \\(N\\) (note that deSolve automatically computes many intermediate time steps as explained in the preamble): logistic_t &lt;- seq(0, 20, 0.1) the initial state of the system (in this case just the value of $N_{0}): logistic_init &lt;- 1 We can then feed all that into a function called lsoda(), which is the only function from the deSolve package that were going to use, and store the result into a variable: logistic_sol &lt;- lsoda(logistic_init, logistic_t, logistic_dyn, logistic_par) Save your file and run all your code so far (click on Source in the top right corner of the script window). In the Workspace window, you should now see the variables youve defined under Values, or your new function logistic_dyn, under Functions. To visualise the contents of the function, click on its name in the Workspace window, or type fix(logistic_dyn) in the Console: this opens a separate window that you must remember to close before you can re-gain access to the command line. You can also look at the contents of the matrix logistic_sol in the same way: the first column is your logistic_t vector (i.e. the time steps) and the second column contains the corresponding values of \\(N(t)\\). 4.1.2.3 Version with names parameters A feature of R that can make your life a bit easier when using lsoda() is the ability to name the elements of a vector. When we defined logistic_par earlier, we had to remember that the first element must be \\(r\\) and the second one \\(K\\). Not too bad with only two parameters, but it can be more of an issue with higher numbers of variables and parameters, as well see next in the next part. Here is an alternative version of the function logistic_dyn which assumes that the third argument, param, is a vector whose elements are named \\(r\\) and \\(K\\): # Calculate the derivative dN/dt using a named vector as its third argument named_logistic_dyn &lt;- function(t,N,par) { # Tell R to use the names of the elements of par (r and K) with(as.list(par),{ return(list(r*N*(1-N/K))) }) } The syntax is a bit complicated because we have to tell R that \\(r\\) and \\(K\\) (which are undefined symbols) will actually be defined within par. Thats the role of the with(x,{...}) statement. A further complication is that with(x,{...}) only works with a named list as its first argument, not a named vector, which is why we wrote with(as.list(par),{...}). As a result, every instruction that comes inside { } in the second argument of with(x,{...}) can make use of the names of the elements of par. Once the function named_logistic_dyn has been defined, it can only be used with a named vector as a third argument. So, in our case, we must define a new vector: named_logistic_par &lt;- c(r=1, K=100) Then you can use lsoda as follows: named_logistic_sol &lt;- lsoda(logistic_init, logistic_t, named_logistic_dyn, named_logistic_par) This will give you exactly the same result as logistic_sol above. So, with this method, you dont need to remember the order of the parameters in the function that calculated the derivative, but you need to remember their names. 4.1.2.4 Plot the dynamics The next step is to plot the dynamics of \\(N(t)\\) using the contents of logistic_sol: plot(logistic_sol[,1], logistic_sol[,2], type=&#39;l&#39;, main=&#39;Logistic growth&#39;, xlab=&#39;t&#39;, ylab=&#39;N&#39;) # Just to check, you can superimpose the graph of the analytical solution of the logistic growth model points(logistic_t,1/(0.01+0.99*exp(-logistic_t)),col=&#39;red&#39;) Task Now plot a few more graphs with different parameter values (re-define logistic_par) or different initial values (logistic_int). Show: Plot 4.1.3 The revenge of the amoeba 4.1.3.1 Code the model Well follow the template described in the previous section, starting with the core function that calculates \\(dN/dt\\) and \\(dP/dt\\). The only difference is that we now have 2 variables, so the function must return a list of two derivatives. There are 5 parameters, so you must be extra careful with the order in which you store them in the vector. To be safe, well enter them in alphabetical order: # Definition of the predation model predation_dyn &lt;- function(t,var,par) { a &lt;- par[1] b &lt;- par[2] d &lt;- par[3] K &lt;- par[4] r &lt;- par[5] N &lt;- var[1] P &lt;- var[2] # Derivatives dN &lt;- r*N*(1-N/K)-a*N*P dP &lt;- -d*P+b*N*P return(list(c(dN,dP))) } # Parameter values, initial variables values and the vector of time values: predation_par &lt;- c(a=0.02, b=0.01, d=0.3, K=100, r=1) predation_init &lt;- c(N=100, P=1) predation_t &lt;- seq(0,40,0.2) # Numerical solution predation_sol &lt;- lsoda(predation_init, predation_t, predation_dyn, predation_par) The resulting matrix has 3 columns: predation_sol[,1] contains the time points, predation_sol[,2] the values of \\(N\\) and predation_sol[,3] the values of \\(P\\). Once again, we only defined the local variables \\(a\\), \\(b\\), \\(d\\), etc. to make it easier to read the code and spot any mistake. But we could write a more concise version using only par[1], par[2], etc. instead. Alternatively, you could use with(as.list(c(par,var)),{ ... }) as we saw in question 2-c above. 4.1.3.2 Plot some graphs Since we have two variables in this model, we can generate more diverse graphs. You should now be getting familiar with plotting instructions, so Ill let you work out how to produce the following graphs. Task Plot \\(N(t)\\) and \\(P(t)\\) against time, together on a graph, using plot(), lines() and legend(). Show: Solution Solution # plot the first line using `plot()` plot(predation_sol[,1], predation_sol[,2], type = &quot;l&quot;, main=&#39;Predation model&#39;, xlab=&#39;t&#39;, ylab=&#39;N.P&#39;, ylim = c(0, 100), col = &quot;blue&quot;) # plot the second line using `lines()` lines(predation_sol[,1], predation_sol[,3], col = &quot;red&quot;) # add legend using `legend()` legend(25, 95, legend=c(&quot;Prey, N&quot;, &quot;Predator, P&quot;),col=c(&quot;blue&quot;, &quot;red&quot;), lty=1, cex=0.8) Task Plot \\(N\\) and \\(P\\) in the phase plane. Use the notes from lecture 2 to find the equations of the nullclines, and draw them using abline(). Finnaly add the equilibrium points. Show: Solution Solution plot(predation_sol[,2], predation_sol[,3], type = &quot;l&quot;, main=&#39;Predation model&#39;, xlab=&#39;N&#39;, ylab=&#39;P&#39;, xlim = c(0, 100), ylim = c(0, 45), col = &quot;black&quot;) # N = 0 abline(v = 0, col = &quot;blue&quot;) # P = 0 abline(h = 0, col = &quot;red&quot;) # P = (r/a) - (rN/Ka) intercept = predation_par[&quot;r&quot;]/predation_par[&quot;a&quot;] slope = -predation_par[&quot;r&quot;]/(predation_par[&quot;K&quot;]*predation_par[&quot;a&quot;]) abline(intercept, slope, col = &quot;blue&quot;) # equilibrium points points(0, 0, pch = 16, cex = 2) points(predation_par[&quot;K&quot;], 0, pch = 16, cex = 2) points( x = predation_par[&quot;d&quot;]/predation_par[&quot;b&quot;], y = (predation_par[&quot;r&quot;]/predation_par[&quot;a&quot;])-((predation_par[&quot;r&quot;]*predation_par[&quot;d&quot;])/(predation_par[&quot;a&quot;]*predation_par[&quot;b&quot;]*predation_par[&quot;K&quot;])), pch = 16, cex = 2 ) 4.1.3.3 Discussion and further investigations Task The model has three equilibrium points, determined by the values of the parameters. In the example above, the dynamics converged to the equilibrium point where the prey and predator coexist. Do you think it would be possible to reach another equilibrium point from different initial conditions? What do the dynamics look like if we start from \\(N(0) = 1\\) and \\(P(0) = 100\\)? Show: Solution Solution Only if you started at another equilibrium point, otherwise this equilibrium is globally stable and will attract all trajectories. predation_init &lt;- c(N=1, P=100) # Numerical solution predation_sol &lt;- lsoda(predation_init, predation_t, predation_dyn, predation_par) # plot the first line using `plot()` plot(predation_sol[,1], predation_sol[,2], type = &quot;l&quot;, main=&#39;Predation model&#39;, xlab=&#39;t&#39;, ylab=&#39;N.P&#39;, ylim = c(0, 100), col = &quot;blue&quot;) # plot the second line using `lines()` lines(predation_sol[,1], predation_sol[,3], col = &quot;red&quot;) # add legend using `legend()` legend(25, 95, legend=c(&quot;Prey, N&quot;, &quot;Predator, P&quot;),col=c(&quot;blue&quot;, &quot;red&quot;), lty=1, cex=0.8) Is that what you expected from a biological point of view? Hopefully! No food for the predators but because this is deterministic, the populations still reach the equilibrium. What limitations of the model does this suggest? No stochastic die out. Task Observe the effects of changing values of \\(b\\) and \\(d\\) on the equilibrium points and the dynamics. What happens if \\(d &gt; bK\\)? Show: Solution Solution In the below figure, \\(b = 0.02\\) and \\(d = 0.01\\). We can see that the prey population essentially dies out. This changes the stability of the equilibrium points so that the stable equilibrium is the one with both populations nead \\(0\\). Task Change the values of \\(b\\) and \\(d\\) simultaneously so that the ratio \\(b/d\\) remains contant. What do you notice? Show: Solution Solution par(mfrow = c(2,2)) b_vec = c(0.01, 0.05, 0.1, 0.2) d_vec = b_vec/(0.01/0.3) for(i in 1:4) { predation_par &lt;- c(a=0.02, b=b_vec[i], d=d_vec[i], K=100, r=1) predation_init &lt;- c(N=100, P=1) predation_t &lt;- seq(0,40,0.2) # Numerical solution predation_sol &lt;- lsoda(predation_init, predation_t, predation_dyn, predation_par) plot(predation_sol[,1], predation_sol[,2], type = &quot;l&quot;, col = &quot;blue&quot;, main = paste(&quot;Predation model&quot;, &quot;b=&quot;, b_vec[i], &quot;,d=&quot;, d_vec[i]), xlab = &quot;t&quot;, ylab = &quot;N.P&quot;, ylim = c(0, 200)) lines(predation_sol[,1], predation_sol[,3], col = &quot;red&quot;) } Similar equilibrium but it takes different times to reach it. "],["epidemic-models---p1.html", "5 Epidemic models - P1 5.1 The basic SIR model 5.2 Coding the model 5.3 Some properties of the model 5.4 Calculating the infectious period 5.5 Optional section - extending SIR to other compartments 5.6 Appendeix - Epidemic peak and final epidemic size", " 5 Epidemic models - P1 5.1 The basic SIR model Task Write down the equations of a frequency-dependent SIR model with no births or deaths and transmission rate \\(\\beta\\) and recovery rate \\(\\gamma\\). Show: Solution Solution \\[\\begin{aligned} \\frac{dS}{dt} &amp;= \\frac{-\\beta SI}{N} \\\\ \\frac{dI}{dt} &amp;= \\frac{\\beta SI}{N} - \\gamma I \\\\ \\frac{dR}{dt} &amp;= \\gamma I\\end{aligned}\\] Task Show that for this model the population size remains constant, i.e. that \\(dN/dt=0\\). Show: Solution Solution \\[\\begin{aligned} \\frac{dN}{dt} &amp;= \\frac{dS}{dt} + \\frac{dI}{dt} + \\frac{dR}{dt} \\\\ &amp;= \\frac{-\\beta SI}{N} + \\frac{\\beta SI}{N} - \\gamma I + \\gamma I \\\\ &amp;= 0 \\end{aligned}\\] 5.2 Coding the model Well follow the template for deSolve that we used in the previous practical, starting with the core function that calculates \\(dS/dt\\), \\(dI/dt\\) and \\(dR/dt\\). Open R, create a new text files and type in: # SIR Epidemic model - Practical 1 library(deSolve) SIR_dyn &lt;- function(t,var,par) { # Rename the variables and parameters S &lt;- var[1] I &lt;- var[2] R &lt;- var[3] N &lt;- S+I+R beta &lt;- par[1] gamma &lt;- par[2] # Derivatives dS &lt;- -beta*S*I/N dI &lt;- beta*S*I/N-gamma*I dR &lt;- gamma*I # Return the 3 values list(c(dS,dI,dR)) } REMEMBER TO SAVE YOUR FILE REGULARLY Note that in this model, the total number of individuals \\(S + I + R\\) remains constant, so we would actually only need to define 2 variables (say \\(S\\) and \\(I\\)) and calculate 2 derivatives. Ive included the 3 variables for clarity only. Then define parameter values, initial variables values and the vector of time values: beta &lt;- 1 gamma &lt;- 0.25 SIR_par &lt;- c(beta,gamma) SIR_init &lt;- c(99,1,0) SIR_t &lt;- seq(0,30,by=0.1) # The numerical solution is given by SIR_sol &lt;- lsoda(SIR_init, SIR_t, SIR_dyn, SIR_par) If you like, you can relabel your variables so that they are easier to use: TIME &lt;- SIR_sol[,1] S &lt;- SIR_sol[,2] I &lt;- SIR_sol[,3] R &lt;- SIR_sol[,4] N &lt;- S + I + R Note: You cannot label time as time in lower case letters because it is an inbuilt function in R. Task Plot \\(S(t)\\), \\(I(t)\\) and \\(R(t)\\) together on a graph, using the commands plot(), lines() and legend(). Show: Solution Solution plot(TIME, S, col = &quot;blue&quot;, type = &quot;l&quot;, ylab = &quot;S, I, R&quot;, xlab = &quot;Time&quot;) lines(TIME, I, col = &quot;red&quot;) lines(TIME, R, col = &quot;darkgreen&quot;) legend(&quot;topright&quot;, legend = c(&quot;Susceptible&quot;, &quot;Infected&quot;, &quot;Recovered&quot;), col = c(&quot;red&quot;, &quot;blue&quot;, &quot;darkgreen&quot;), pch = c(&quot;-&quot;,&quot;-&quot;, &quot;-&quot;) ) You dont have to always plot things against time, it can also be useful to plot variables against each other. Try plotting \\(S\\) against \\(I\\): plot(S/N, I/N, type = &quot;l&quot;, lwd = 3, col = &quot;darkgreen&quot;) It can be difficult to read this graph at first. Find where \\(\\text{TIME} = 1\\) (at the start \\(S/N \\approx 1\\) and \\(I \\approx 0\\)) and label it on the graph. index &lt;- which(TIME == 1) text(S[index]/N[index],I[index]/N[index],&quot;t=1&quot;,pos=2) Task Also label \\(t=2\\), \\(t=5\\), \\(t=10\\) and \\(t=15\\). What is happening to time in this plot? Show: Solution Solution Approaching the steady state. 5.3 Some properties of the model An important question is when does the epidemic reach its peak? A common misconception based on graphs similar to the one above is \\(I(t)\\) reaches its maximum when \\(S(t) = R(t)\\). By now you know that \\(I(t)\\) reaches its maximum when \\(dI/dt=0\\). Given that \\(dI/dt=\\beta SI/N-\\gamma I\\), when \\(I\\) is maximum we get \\(S/N=\\gamma /\\beta\\). Check this relationship using the graph of \\(S/N\\) plotted against \\(I/N\\). You might want to use which.max() which gives the position of the largest element in a vector. Tip: you can use abline to mark on the plot where \\(I/N\\) reaches its maximum. abline(v=S[which.max(I)]/N[which.max(I)], col =&quot;black&quot;) Next, lets look at the effects of \\(\\beta\\) and \\(\\gamma\\) on the dynamics of the system. In practice, \\(\\beta\\) is usually not known before an epidemic, so its estimated in the early stages of an epidemic using the average infectious period \\((1/\\gamma)\\) and \\(R_{0}\\). Instead of calculating and plotting a series of dynamics manually, well use a loop to plot 6 graphs in a single window. Task Disease \\(R_{0}\\) Infectious period, \\(1/\\gamma\\) (days) \\(\\beta\\) Peak size Peak time Epidemic size Flu (H1N1p) 1.2 1 Flu 1.5 1 SARS 2.0 28 Smallpox 5.0 5 Measles 17.0 7 Pertussis 17.0 14 Fill in the blanks and excute the following instructions: # set up a 2 by 3 grid for the plots: par(mfrow=c(2,3), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;) # define the parameters: infperiod &lt;- c(1,1,28,5,7,14) Rzero &lt;- c(1.2,1.5,2,5,17,17) for (i in 1:6) { gamma = signif(1/infperiod[i],2) beta = signif(Rzero[i]*gamma,2) SIR_par &lt;- c(beta,gamma) SIR_sol&lt;-lsoda(SIR_init, SIR_t, SIR_dyn, SIR_par) plot(....., ylim=c(0,1),type=&quot;l&quot;, main=paste0(beta = ,beta,gamma = ,gamma)) lines(.....) # I(t) lines(.....) # R(t) } Show: Solution Solution # set up a 2 by 3 grid for the plots: par(mfrow=c(2,3), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;) # define the parameters: infperiod &lt;- c(1,1,28,5,7,14) Rzero &lt;- c(1.2,1.5,2,5,17,17) SIR.t =seq(0,500, by=0.1) for (i in 1:6) { gamma = signif(1/infperiod[i],2) beta = signif(Rzero[i]*gamma,2) SIR_par &lt;- c(beta,gamma) SIR.sol &lt;- lsoda(SIR_init,SIR_t,SIR_dyn,SIR_par) TIME &lt;- SIR.sol[,1] S &lt;- SIR.sol[,2] I &lt;- SIR.sol[,3] R &lt;- SIR.sol[,4] N &lt;- S + I + R plot(TIME,S/N,col=&#39;blue&#39;, type=&#39;l&#39;, ylim=c(0,1), main=paste(&quot;beta = &quot;,beta,&quot;gamma = &quot;,gamma)) lines(TIME,I/N,col=&quot;red&quot;) lines(TIME,R/N,col=&quot;green&quot;) legend(&quot;right&quot;, col = c(&quot;blue&quot;,&quot;red&quot;,&quot;green&quot;),legend = c(&quot;S&quot;,&quot;I&quot;,&quot;R&quot;), lty = 2) print(paste(&quot;peak size = &quot;,max(I/N))) print(paste(&quot;peak time = &quot;,TIME[which.max(I/N)]) ) print(paste(&quot;Epidemic size = &quot;,max(R/N))) } ## [1] &quot;peak size = 0.0231066389917149&quot; ## [1] &quot;peak time = 8.1&quot; ## [1] &quot;Epidemic size = 0.344611420022074&quot; ## [1] &quot;peak size = 0.0697227613470483&quot; ## [1] &quot;peak time = 6.5&quot; ## [1] &quot;Epidemic size = 0.593574217769393&quot; ## [1] &quot;peak size = 0.0278724769684512&quot; ## [1] &quot;peak time = 30&quot; ## [1] &quot;Epidemic size = 0.0189539739504256&quot; ## [1] &quot;peak size = 0.480101096709578&quot; ## [1] &quot;peak time = 7.7&quot; ## [1] &quot;Epidemic size = 0.982652741440976&quot; ## [1] &quot;peak size = 0.776354567432811&quot; ## [1] &quot;peak time = 3.3&quot; ## [1] &quot;Epidemic size = 0.979907643827461&quot; ## [1] &quot;peak size = 0.774111948146292&quot; ## [1] &quot;peak time = 6.7&quot; ## [1] &quot;Epidemic size = 0.840041300875431&quot; For each combination of parameter values, write down \\(\\beta\\) and read off the approximate values of the height and time of the epidemic peak and the final epidemic size (the total number of individuals that were infected). (Hint: you may have to extend the time period!) You can also use the functions max() and which.max(). Remember from the lecture that \\(R_{0}=\\beta / \\gamma\\), therefore \\(\\beta = \\gamma R_{0}\\). Task Which of the last 3 quantities appear linked to \\(R_{0}\\)? Show: Solution Solution Peak size and epidemic size. Note: It can be shown mathematically that the epidemic size and the height of the epidemic peak are functions of \\(R_{0}\\) only, although this requires some clever manipulation of the ODES (see Appendix at the end of this handout if youre interested). The time to the epidemic peak cannot be directly expressed from the equations, but it is linked to the initial rate of spread of the epidemic. The variations in the number of infected individuals are given by: \\[dI/dt = I(\\beta S/N-\\gamma)\\] At the very beginning of the outbreak, \\(I(t) &lt;&lt; S(t) \\approx N\\). So we can approximate the ODE above as \\(dI/dt = (\\beta - \\gamma )I\\), which can be solved as: \\[ I(t) = I_{0} exp(\\beta - \\gamma)t\\] So the number of infected individuals initially follows an exponential growth with rate \\[ \\beta N - \\gamma = \\gamma (R_{0} - 1) \\] Task Plot \\(log(I(t))\\) for \\(t\\) in \\([0,4]\\) for the same parameter combinations above and check that the initial gradient is equal to \\(\\beta - \\gamma\\). TIP: you can use abline again, this time to impose a straight line \\((y=bx + c)\\). Show: Solution Solution #checking initial gradient par(mfrow=c(2,3)) # define the parameters: infperiod &lt;- c(1,1,28,5,7,14) Rzero &lt;- c(1.2,1.5,2,5,17,17) SIR.t =seq(0,500, by=0.1) for (i in 1:6) { gamma = signif(1/infperiod[i],2) beta = signif(Rzero[i]*gamma,2) SIR_par &lt;- c(beta,gamma) SIR.sol &lt;- lsoda(SIR_init,SIR_t,SIR_dyn,SIR_par) TIME &lt;- SIR.sol[,1] S &lt;- SIR.sol[,2] I &lt;- SIR.sol[,3] R &lt;- SIR.sol[,4] N &lt;- S + I + R plot(TIME[1:41],log( I[1:41]/N[1:41] ),col=&#39;red&#39;, type=&#39;l&#39;, main=paste(&quot;beta = &quot;,beta,&quot;gamma = &quot;,gamma)) abline(a = log( I[1]/N[1] ), b = beta-gamma, col = &quot;black&quot;) } Task What dynamics do you expect with \\(R_{0} = 1\\)? Plot the graph for \\(\\beta = 1\\), \\(\\gamma = 1\\). Why does \\(I(t)\\) decreases? Show: Solution Solution As our initial conditions are with \\(99\\) susceptible people, and \\(R_{0}=1\\) this renders the effective reproduction number less than \\(1\\) so $I(t) decreases. Task Explore the behaviour of the system around \\(R_{0} \\approx 1\\) using the loop you wrote before. Try changing \\(0.9 &lt; R_{0} &lt; 1.1\\) and altering the initial number of individuals infected. Show: Solution Solution ### looking around R0 = 1 par(mfrow=c(2,3)) # define the parameters: SIR.init &lt;- c(99,1,0) infperiod &lt;- rep(2,6) Rzero &lt;- c(0.9,0.95,1,1.05,1.1, 1.15) SIR.t =seq(0,100, by=0.1) for (i in 1:6) { gamma&lt;- signif(1/infperiod[i],2) beta &lt;- signif(Rzero[i]*gamma,2) SIR_par &lt;- c(beta,gamma) SIR.sol &lt;- lsoda(SIR_init,SIR_t,SIR_dyn,SIR_par) TIME &lt;- SIR.sol[,1] S &lt;- SIR.sol[,2] I &lt;- SIR.sol[,3] R &lt;- SIR.sol[,4] N &lt;- S + I + R plot(TIME, I/N,col=&#39;red&#39;, type=&#39;l&#39;, ylim=c(0,0.03), main=paste(&quot;R0 = &quot;,Rzero[i])) } 5.4 Calculating the infectious period If the recovery rate of a disease is \\(\\gamma = 0.2\\) days\\(^{-1}\\), what is the average infectious period? For a given constant recovery rate of \\(\\gamma = 0.2\\) days\\(^{-1}\\) plot the proportion of individuals still infected over time. # set up a vector of times: x=seq(0,30,0.1) # define the recovery rate, gamma: gamma=0.2 # define the exponential function: fx=exp(-gamma*x) #plot: plot(x,fx,type=&quot;l&quot;) Calculate the average infectious period using the function defined above. print(sum(fx*x)/sum(fx)) Task How do your two answers compare? Show: Solution Solution Not quite the same but perhaps will have a better approximation as you use more points. 5.5 Optional section - extending SIR to other compartments As we saw in the lecture, the S-I-R framework is only a crude representation of the real natural history of a disease. Think of another formulation and see how the results compare to a standard SIR model. Some suggestions: Include loss of immunity, so that Recovered individuals become Susceptible again after a certain period of time. Include a latent state post-infection but before an individual becomes infectious (this type of model is often called an SEIR model, where E stands for the Exposed class). Include treatment in the model, allowing Infected individuals to recover more quickly. Sketch a diagram of your model, showing the compartments and the flows between them. Write down the equations that govern your model Write a function, such as SEIR_dyn() that calculates the derivatives of your model for use with lsoda(). Plot the epidemic curve, \\(I(t)\\). How does this compare to an equivalent SIR model? Show: Solution Solution #### Extra exercise # I am putting everything in at once SEITRS.dyn &lt;- function(t, var, par) { # Rename the variables and parameters S &lt;- var[1] E &lt;- var[2] I &lt;- var[3] tr &lt;- var[4] R &lt;- var[5] N &lt;- S + E + I + tr + R beta &lt;- par[1] gamma &lt;-par[2] epsilon &lt;- par[3] #rate of latency loss tau &lt;- par[4] #treatment rate omega &lt;- par[5] #recovery rate for treated people # Derivatives dS &lt;- -beta*S*I/N dE &lt;- beta*S*I/N - epsilon*E dI &lt;- epsilon*E - gamma*I - tau*I dT &lt;- tau*I - omega*tr dR &lt;- gamma*I + omega*tr # Return the 5 values list(c(dS, dE, dI, dT, dR)) } beta &lt;- 2 gamma &lt;- 0.25 epsilon &lt;- 1 tau &lt;- 1 omega &lt;- gamma*2 SEITRS.par &lt;- c(beta,gamma,epsilon,tau,omega) SEITRS.init &lt;- c(99,1,0,0,0) SEITRS.t &lt;- seq(0,30,by=0.1) SEITRS.sol &lt;- lsoda(SEITRS.init,SEITRS.t,SEITRS.dyn,SEITRS.par) TIME &lt;- SEITRS.sol[,1] S &lt;- SEITRS.sol[,2] E &lt;- SEITRS.sol[,3] I &lt;- SEITRS.sol[,4] tr &lt;- SEITRS.sol[,5] R &lt;- SEITRS.sol[,6] N &lt;- S + E + I + tr + R plot(TIME,S/N,col=&#39;blue&#39;, type=&#39;l&#39;, ylim=c(0,1)) lines(TIME,E/N,col=&quot;orange&quot;) lines(TIME,I/N,col=&quot;red&quot;) lines(TIME,tr/N,col=&quot;cyan&quot;) lines(TIME,R/N,col=&quot;green&quot;) legend(&quot;right&quot;, col = c(&quot;blue&quot;,&quot;orange&quot;, &quot;red&quot;,&quot;cyan&quot;,&quot;green&quot;), legend = c(&quot;S&quot;,&quot;E&quot;,&quot;I&quot;,&quot;T&quot;,&quot;R&quot;), lty = 2) Plenty to note on the different dynamics as you experiment with the above- one thing to examine is that we now have multiple infected states and this can make calculating \\(R_{0}\\) more complicated as we need to use the next generation matrix approach, shown later in the course. 5.6 Appendeix - Epidemic peak and final epidemic size Although it is not possible to solve the differential equations of the SIR system analytically to obtain expressions for \\(S(t)\\), \\(I(t)\\) and \\(R(t)\\), we can obtain an implicit solution in the \\((S,I)\\) phase plane. In other words, we can obtain a mathematical relationship between the values of \\(S\\) and \\(I\\) at any time point. The trick is to re-write the system \\[\\begin{aligned} \\frac{dS}{dt} &amp;= -\\beta SI/N \\\\ \\frac{dI}{dt} &amp;= \\beta SI/N - \\gamma I \\end{aligned}\\] as a single differential equation for \\(I\\) as a function of \\(S\\). Under certain mathematical conditions that we wont discuss here, the variations of \\(I\\) with respect to \\(S\\) can conveniently be written as: \\[\\frac{dI}{dS} = \\frac{dI/dt}{dS/dt} = \\frac{\\beta SI/N - \\gamma I}{-\\beta SI/N} = \\frac{\\gamma N}{\\beta S} - 1\\] Using the integration rules from the maths lectures, we can actually solve this ODE: So, given the initial conditions, we can deduce the value of \\(I(t)\\) from the value of \\(S(t)\\) at any time point. How is that helpful? First consider the question of the height of the epidemic peak. We know that \\(I(t)\\) reaches its maximum when \\(S(t) = \\gamma/\\beta\\). Hence the height of the epidemic peak: \\[I_{max} = I_{0} + \\frac{\\gamma}{\\beta} log \\frac{\\gamma}{\\beta S_{0}} + S_{0} - \\frac{\\gamma}{\\beta}\\] Assuming that \\(I(0) &lt;&lt; S(0) \\approx N\\), then we can write \\(R_{0}\\approx\\beta S(0)/\\gamma\\), hence: \\[I_{max}/N \\approx 1-(1+logR_{0})/R_{0}\\] You can check it on the graphs from section 2 above. Next, the final epidemic size is given by the final valuep of \\(S(0) - S(t)\\), i.e. taking \\(t \\rightarrow \\infty\\). Under the same assumptions as above, and letting \\(x = S(0) - S(\\infty)\\) be the proportion of the \\(N\\) population that gets infected over the whole epidemic, we get the following equation: \\(x + log(1 - x)\\), which can only be solved numerically. R does not have a built-in numerical solver for nonlinear equation, but it has a function called optimize() that searches for the minimum of a numerical function. The trick is to notice that solving an equation of the form \\(f(x) = 0\\) is equivalent to finding the minimum of \\(f^{2}(x)\\), the square of \\(f(x)\\). If youre not convinced, sketch a graph of an arbitrary function that crosses the x-axis, then sketch the square of that function. We can therefore estimate the epidemic size for any value of R0 using the following code: my_fun &lt;- function(x,R0) { (x+log(1-x)/R0)2 } optimize(my_fun, c(0,1), R0=2) where the three arguments of optimize are: the function to minimize with respect to its first argument (in this case \\(x\\)), - the interval over which to search for a solution the numerical values of any other argument of my_fun (in this case \\(R_{0}\\)). Well see more about optimize() next week, so I wont give any more detail here. "],["epidemic-models---p2.html", "6 Epidemic models - P2 6.1 Calculating \\(R_{0}\\)", " 6 Epidemic models - P2 6.1 Calculating \\(R_{0}\\) We wish to calculate \\(R_{0}\\) which is defined as the expected number of secondary infections per generation given one infected individuals is introduced to an entirely susceptible population. Mathematically, we examine our system equations for the infectious compartments only* at disease free equilibrium** and see how they *change** as each infectious variable changes. We divide this into two situations: Transmission events where a new infectious variable is created eg. a susceptible person be-comes infected Transition events where an infectious variable is lost eg. an infected person recovers. SIR example: \\[ \\frac{dS}{dt} = \\mu H - \\beta SI - \\mu S\\] \\[ \\frac{dI}{dt} = \\beta SI - \\mu I - \\gamma I\\] \\[ \\frac{dR}{dt} = \\gamma I - \\mu R\\] We only have one infectious variable: \\(I\\). So our system of infectious compartments is only the equation for the change in the infected population over time. \\[ \\frac{dI}{dt} = \\beta SI - \\mu I-\\gamma I\\] Next we wish to know how this equation varies as the infectious variable, \\(I\\), varies. To do this we _____________________ with respect to \\(I\\) to get: Task We are interested in our system at disease free equilibrium, so wherever we see a variable, we replace it with its equilibrium value. At disease free equilibrium: \\(S^{*}=H\\), \\(I^{*}=0\\), \\(R^{*}=0\\). Task Now we can divide our equation into transmission events and transition events. Transmission terms: Task Transition terms: Task Finally, \\(R_{0}\\) is calculated by dividing the Transmission terms by (- Transition terms). The intuition is that \\(1/\\text{Transition}\\) is equal to the generation time. Thus, we arrive at the transmission events per generation. Task \\(R_{0} =\\) "],["epidemic-models---p3.html", "7 Epidemic models - P3 7.1 Coding a simple SI model 7.2 Dissecting the stochastic SI code 7.3 Performing Multiple Simulations 7.4 Exploring the effects of Stochasticity 7.5 The delay time 7.6 Coding a simple SIR model 7.7 SIS Extension: Adding Recovery from Infection 7.8 SIS Extension: Adding host demography 7.9 SIR Extension: The final epidemic size 7.10 SIR Extension: Stochastic extinction", " 7 Epidemic models - P3 Stochastic models Aim: To explore the effects of stochasticity on simple SI and SIR models and contrast their behaviour with previous deterministic results. Goals: By the end of this practical participants should be able to achieve the following: Code up a simple stochastic SI model in R using the SimInf package Code up a simple stochastic SIR model in R using the SimInf package Run both single and multiple realisations and visualise the results Explore extensions to both SI and SIR models Expand the SI model to include additional transitions Understand the impact of R0 values on stochastic extinction Background: This practical follows on from the Stochastic Epidemic Models Lecture 5 and echoes the content presented there. 7.1 Coding a simple SI model We will first implement a slight modification to the code that has been presented to you in the previous Stochastic Epidemic Lecture (Epidemics L5). The key difference between the code presented in the lecture and the code shown here is that we are putting the code into a function (called StocSI.dyn) rather than just running a series of commands. Open Rstudio Open a new blank script file Save it as Stock_SI.R If you havent already instealled the package SimInf: Click packages tab in lower right sub-window Click Install Type: SimInf in the Packages box Click Install If youre on a Windows computer then youll need to do some additional work to use SimInf (macOs and Linux users should be fine) Download Rtools from https://cran.r-project.org/bin/windows/Rtools/ Click on the appropriate version of RTools on the left For RTools 4.2 (the most up to date version) click on the link Rtools42_installer. Its in the third paragraph down in the second section Once its downloaded install it using all of the default settings This might take a couple of minutes, but click Finish when its done Add the path to Rtools to your PATH environment variable Click on the windows icon in the bottom right of the screen Type environment variable in the search box This should bring up the System Properties window. Click on the Environment Variables button Select on the Path row in the second table (entitled System Variables) and click Edit Click New Type C:\\Rtools\\bin Click OK on all windows Great! Now we can finally actually get on with the simulations. In the script window type the following commands: require(SimInf) create.StocSI &lt;- function(N, I0, beta , f_time) { initial_state &lt;- data.frame(S = N-I0 , I = I0) compartments &lt;- c(&quot;S&quot;, &quot;I&quot;) transitions &lt;- c(&quot;S -&gt; beta*S*I/(S+I) -&gt; I&quot;) tspan &lt;- seq(from = 1, to = f_time, by = 1) model &lt;- mparse(transitions = transitions, compartments = compartments, gdata = c(beta = beta), u0 = initial_state, tspan = tspan) return(model) } Make sure you save your file and that your working directory is the same as the folder you have saved this into (Choose from menu Session &gt; Set Working Directory &gt; To Source File Location). This code creates a function that creates a single stochastic SI model. Well dissect/explain the code in the next section. Open a new blank script file. Save it as Epi_P3.R Well use this script to record all of the steps in this practical (unless otherwise indicated). In the script window type the following commands and then run (select and press run button in RStudio): source(&#39;Stoch_SI.R&#39;) SImodel &lt;- create.StocSI(500, 1, 0.5, 30) out &lt;- run(model = SImodel) plot(out) You should see a graph that looks similar to this: Here we can see the two trajectories for an SI epidemic. The susceptible curve (red) starts at 499 and stochastically decreases down whilst the infected curve (blue) starts at 1 and mirrors the S curve increasing stochastically. 7.2 Dissecting the stochastic SI code The function we have just created is a wrapper that takes the basic parameters that define the SI model for a particular population and creates a R object that the SimInf package can use to simulate the model. Our function create.StocSI takes four inputs: Total population size: N Initial number of infected: I0 Infection rate: beta Time to simulate for: f_time Inside the function we combine this information to create four R objects that when passed to the SimInf function mparse. The output from the mparse function is returned as the output to our create.StocSI function. Here we will break down what we have just written and explain what is required for the mparse function. Note: The SimInf package includes its own function that defines SIR, SEIR and SIS models (of which the SI model is a special case). The mparse function can define any generic compartmental model so it is worth learning about The mparse function requires that you specify the following information: The list of all compartments of the system The initial states (or compartments) of the system the events (or transitions) of the system the values of any parameters the time points where the value of each state is to be recorded for each simulation An SI model is described by the following system of differential equations: \\[\\frac{dS}{dt} = -\\beta\\frac{SI}{N}\\] \\[\\frac{dI}{dt} = \\beta\\frac{SI}{N}\\] And from these equations we can derive a transition diagram from the system: \\[ S \\rightarrow \\beta\\frac{SI}{N} \\rightarrow I\\] From this diagram we can see that there are two states for this system: susceptible and infected (S &amp; I). We define all of the compartments of the model using a vector: compartments &lt;- c(&quot;S&quot;,&quot;I&quot;) In order to begin the simulation, we need to specify the initial values of each compartment. We specify these using a data frame with a single row and column for each compartment specified above: initial_state &lt;- data.frame(S = N-I0, I = I0) where N (the total population number) and I0 (the initial number of infected) have been specified by the user. There is one event (or transition): infection. transitions &lt;- c(&quot;S -&gt; beta*S*I/(S+I) -&gt; I&quot;) where the effect is to move one susceptible into the infected compartment at a rate \\(\\beta \\frac{SI}{N}\\) where \\(N=S+I\\) and beta must be specified by the user. The syntax we use here should be self-explanatory (hopefully) but the idea is that we have the following syntax A \\(\\rightarrow\\) rate \\(\\rightarrow\\) B where A is the name of the starting compartment, B is the name of the ending compartment and rate explains how to calculate the rate. We must tell algorithm how long we want it to run for before stopping and at what points we wish to record the value of the compartments. We use the user specified variable f_time to specify a sequence of daily output values up to a maximum value of f_time. tspan &lt;- seq(from = 1, to = f_time, by = 1) NB: SimInf was designed to work with real epidemiological data and as such the minimum output timestep size is 1 (corresponding to 1 day). The mparse function takes the arguments weve just created, processes (parses) them into a low-level computer language and compiles the resulting program into machine code. This machine code is considerably (orders of magnitude) faster than the equivalent R code. We assign the output of this function to the object SImodel (An object in R is a special data type that combines functions with data). Objects can be used as arguments to functions in the same way as normal data types. In this case we can use the run() function to run a stochastic simulation of the SImodel: out &lt;- run(model = SImodel, threads = 1) The threads argument is optional here, but this argument would allow us to parallelise our code (if our setup allows it). The run function implements (and runs) the stochastic simulation algorithm for any stochastic model defined by the mparse function and returns a new object which contains the output of the simulation (which weve called out). The SimInf package provides a special plot function for this output plot(out) This explains the code youve produced so far, but there are limitations. We would like to plot multiple stochastic realisations onto a single plot and investigate their aggregated properties. To do this we will need to modify our code somewhat. 7.3 Performing Multiple Simulations Lets update our create.StocSI function in the (Stoch_SI.R file) to create a model with multiple copies of the same population. This is the simplest possible meta-population model where there is no interaction between patches, and a neat hack to run multiple replicate simulations at the same time. Edit Stoch_SI.R to update the definition of the create.StocSI as follows: create.StocSI &lt;- function(N, I0, beta, f_time, reps = 1) { initial_state &lt;- data.frame(S = rep(N-I0, reps) , I = rep(I0, reps)) compartments &lt;- c(&quot;S&quot;, &quot;I&quot;) transitions &lt;- c(&quot;S -&gt; beta*S*I/(S+I) -&gt; I&quot;) tspan &lt;- seq(from = 1, to = f_time, by = 1) model &lt;- mparse(transitions = transitions, compartments = compartments, gdata = c(beta = beta), u0 = initial_state, tspan = tspan) return(model) } The only two things weve changed here are: Weve created a new argument call reps which were using to specify the number of simulations that we want the code to run. Weve changed how we specify the initial_state dataframe. This is now a dataframe that has reps number of rows and 2 columns. The first column contains the starting number of susceptible individuals in each of our simulations, and the second column contains the number of initial infected individuals. Again, remember to save your file. Note: we have added a new argument to create.StocSI which has a default value reps\\(=1\\). By adding the argument in this way we ensure that any code we wrote earlier assuming only one replicate would still work with no modifications. Type in the following code into Epi_P3.R and run: #Multiple replicates source(&#39;Stoch_SI.R&#39;) SImodel &lt;- create.StocSI(500, 1, 0.5, 30, 20) out &lt;- run(model = SImodel) plot(out) This should produce a plot that looks similar to the following: By default the plot command will always: plot the curves for all states on the same graph (so we can see both S and I curves here) use a step function to plot the curves (which is why its a bit, um, step-y) show the average and interquartile range as a transparent envelope when multiple simulations have been run. If we want to actually see individual realisations, we need to use some of the other arguments to the plot function. Try typing this in: plot(out , &#39;I&#39;, range=FALSE , type = &#39;l&#39;) This should produce a plot that look similar to the following: The second argument is a character vector specifying which compartments we want to see. Here weve asked to only see the curves for the infected I compartment. The named argument range is used to specify the shaded quantile on the plot. By specifying FALSE, weve turned it off and forced R to plot the individual trajectories instead. If wed specified a numeric value between 0 and 1 then we would see the equivalent shaded range plotted instead (i.e. \\(range=0.5\\) corresponds to plotting the interquartile range and \\(range = 0.95\\) corresponds to plotting the wider 95% quantiles) We can use the standard type argument from the base R plot function to specify the type of curve we want. The default is type='s' for steps, but here were asking for the smoother type='l' lines option. 7.3.1 Adding the deterministic solution curve There will be an R script file called Det_SI.R on the shared drive (your demonstrator will know where). It contains the code for a deterministic SI model based upon the code you encountered in the first epidemic models practical. It is specifically designed to use the same parameters as have been used in the stochastic SI code written above and will add a deterministic trajectory to your plot. Type in the following: #Adding a deterministic trajectory source(&#39;Det_SI.r&#39;) det.sol &lt;- DetSI.dyn(500, 1, 0.5, 30) det.t &lt;- det.sol[,1] det.I &lt;- det.sol[,3] lines(det.t, det.I, lwd=3 , col=&#39;red&#39;) 7.4 Exploring the effects of Stochasticity Task Exercise 1 Modify the above code to explore the effects of different initial conditions ($I_{0}$ = 1, 5, 10, 50, 100) on the stochastic replicates. Keep the total population size constant (\\(N=500\\)). Qualitatively what effect does this have on the variability between replicates? Task Exercise 2 Now vary the population size (N = 50, 500, 5000) with \\(I_{0} = 1\\). Qualitatively what effect does this have on the variability between replicates? Task Exercise 3 Now vary the infection rate \\((\\beta = 0.1, 0.5, 1)\\). Qualitatively what effect does this have on the variability between replicates? 7.5 The delay time Graphical exploration of model output is a good way to orientate yourself and understand the qualitative impact of changing parameters, but in practice we do need to explore various summary statistics. In this section we will compare the time it takes for a stochastic epidemic to infect half of the total population, \\(t_{\\frac{1}{2}}\\), with the time a deterministic epidemic takes. This difference is the delay time, \\(\\delta t\\) (see the figure below). We will repeat this for many stochastic replicates and plot a distribution of the delay times. 7.5.1 The deterministic half-time The deterministic model predicts a constant value of \\(t_{\\frac{1}{2}}\\) given by the expression: \\[t_{\\frac{1}{2}} = -\\frac{1}{\\beta}ln\\frac{I_{0}}{N-I_{0}}\\] (This can be derived exactly from the analytic solution to the system of differential equations for the SI system. This isnt going to be possible for more complex models). 7.5.2 The stochastic half-time We must now construct a function that calculates the value of \\(t_{\\frac{1}{2}}\\) for a set of stochastic simulations and the corresponding delay times: Open a new script file and save as Half_time.R with the following content: # Deterministic Halftime Function det_halftime &lt;- function(N, I0, beta) { # calculate deterministic half life return((-1/beta)*log(I0/(N-I0))) } # Stochastic Halftime Function stoc_halftime &lt;- function(model) { no_runs = n_nodes(model) N = sum(model@u0[,1]) # create storage vector for stochastic half lives thalf &lt;- numeric(no_runs) # loop over number of replicates for (iSim in 1:no_runs) { # extract the infected time series for each simulation output &lt;- trajectory(out, &quot;I&quot;, iSim) # find the first time when the infected population is at least half the total population ind &lt;- min(which(output$I&gt;=(N/2))) thalf[iSim] &lt;- output$time[ind] } return(thalf) } The function det_halftime takes three arguments: The total host population N The initial number of infected I0 The infection rate beta and returns the deterministic value of the time to half-height. The function stoc_halftime takes a SimInf model object as its input and returns a vector with the time to half height of each model simulation (to nearest day) We then plot a probability density plot for these delay times using the in-built function density. source(&#39;Half_time.R&#39;) SImodel &lt;- create.StocSI(500, 1, 0.5, 30, 100) out &lt;- run(model = SImodel) delay_times &lt;- stoc_halftime(out) - det_halftime(500, 1, 0.5) plot(density(delay_times), main = &#39;&#39;) density produces smoothed estimates of the frequency distributions of a set of samples (essentially a smooth line through the top of a histogram). This will produce a figure similar to the one below: Task Exercise Can the distribution of delay-times be used to quantify the appropriateness of the deterministic approximation for a given set of parameters? In particular, in what limits will the deterministic model produce useful predictions for the time-scale of an SI epidemic? You may wish to revisit the impact of changing the host-population size N, initial number of infectives \\(I_{0}\\) and infection rate. Produce plots to justify your reasoning and discuss them with a demonstrator. 7.6 Coding a simple SIR model Using the results from the earlier section on SI models as a starting point we will construct a stochastic SIR model. The deterministic SIR model equations are given by \\[\\frac{dS}{dt} = -\\beta\\frac{SI}{N}\\] \\[\\frac{dI}{dt} = \\beta\\frac{SI}{N}-\\mu I\\] \\[\\frac{dR}{dt} = \\mu I\\] 7.6.1 Exercise one Using these equations construct a transition diagram for the system What are the states/compartments for this system? What are the transitions events and what are the rates associated with each event? What is the initial state of the system (i.e. how many individuals should be in each compartment initially)? One a new script in RStudio. Using the functions developed in the previous sections as a template construct a create.StocSIR function that takes five arguments: N: the total population size I0: the initial number of infected individuals beta: the infection rate mu: the recovery rate f_time: the time the simulation will run for (daily steps) reps: the number of replicate simulations to run The skeleton function below can serve as a starting point. create.StocSIR &lt;- function(N, I0, beta, mu, f_time, reps = 1) { initial_state &lt;- compartments &lt;- transitions &lt;- tspan &lt;- model &lt;- mparse(transitions = transitions, compartments = compartments, gdata = c(beta = beta, mu = mu), u0 = initial_state, tspan = tspan) return(model) } Hint To specify multiple transitions in the SimInf algorithm you will need a transitions vector with two elements to it like this: tansitions &lt;- c(&#39;S -&gt; ?? -&gt; I&#39;, &#39;I -&gt; ?? -&gt; R&#39;) where I have deliberately replaced the relevant transition rates with ??. You will need to replace the ?? with the correct rate equations. Using your function run the following code: SIRmodel &lt;- create.StocSIR(500, 1, 1, 0.25, 30) out &lt;- run(model = SIRmodel) plot(out) This should produce a graph that looks like this: However, some of you will see a very different graph. (If you do get something like this plot then try rerunning the model and plotting it several times until you see something different.) Why might this be the case? 7.6.2 Exercise two Use this function to plot 50 replicates of the infection curves (use the code in the previous SI section as a guide. This should produce a graph that looks like this: 7.6.3 Exercise three Load the R script called Det_SIR.R and adapting your code from the previous section make a plot that overlays the deterministic solution with the output of the stochastic model. 7.7 SIS Extension: Adding Recovery from Infection Consider a system where individuals recover from infection with a fixed rate \\(\\alpha\\) and return to the susceptible class. This SIS system is described by the following system of equations: \\[\\begin{aligned} \\frac{dS}{dt} &amp;= \\alpha I - \\beta\\frac{SI}{N} \\\\ \\frac{dI}{dt} &amp;= \\beta\\frac{SI}{N} - \\alpha I \\end{aligned}\\] Task Draw the transition diagram for this system. For each transition write down the rate at which it occurs. What are the parameters required for this model? Write a new function create.StocSIS based upon the create.StocSI function and add to the Stoch_SI.R file and repeat the above analysis. 7.8 SIS Extension: Adding host demography Consider an SI model with host demography. The system has constant birth rate \\(\\Lambda\\) into the susceptible class and fixed per capita mortality rate \\(\\mu\\) from both the susceptible and infectious classes and is governed by the following equations: \\[\\frac{dS}{dt} = \\Lambda - \\beta\\frac{SI}{N} - \\mu S\\] \\[\\frac{dI}{dt} = \\beta\\frac{SI}{N}-\\mu I\\] Task Repeat the previous steps for this model and create a create.StocSI_demo function in Stoch_SI.R and then explore its dynamics. Hint To specify a birth (or death) event then we use the @ symbol to represent the empty compartment from which births appear (or to which deaths end up). So a birth event into the susceptible compartment would have the following transition string: &#39;@ -&gt; ?? -&gt; S&#39; Use these initial values for the parameters: alpha \\(= 0.1\\) beta \\(=0.5\\) lambda \\(=100\\) 7.9 SIR Extension: The final epidemic size In order to further explore the effects of stochasticity in the SIR model we are going to explore the how the final size of an epidemic varies between realisations. 7.9.1 Exercise one Task What is the final size of an epidemic? What does it mean? Which state variable do we need to consider in order to calculate the final epidemic size? For stochastic simulations the final epidemic size will vary between realisations of the epidemic which will mean that we need to think about a distribution of final epidemic sizes. 7.9.2 Exercise two Task Write code to calculate the final epidemic size from a SimInf output object. Hint What value should the final time for the simulations be set to (roughly)? As weve seen before, the function trajectory can be used to extract the actual values of the different compartments at each time point. Look at the code we used in the previous practical for calculating the halftime values for a single simulation and modify that logic accordingly. 7.9.3 Exercise three Task Run 100 realisations, extract the final epidemic size from each one and then produce a histogram of the final sizes. It should look something similar to the figure below: Hint Youll probably want to use a loop to extract the final size for each simulation Set the histogram to use 500 bins Why does the histogram have a bimodal distribution? 7.9.4 Exercise four Recall that the final epidemic size for the deterministic SIR model showed strong dependency on the value of the basic reproductive number, \\(R_{0}\\). Task Write down an expression for \\(R_{0}\\) for this system as a function of the parameters \\(\\beta\\) and \\(\\mu\\). Does this help explain the bimodal result? 7.9.5 Exercise five For this section, we shall vary the value of \\(R_{0}\\) by varying the transmission rate, \\(\\beta\\), whilst keeping the recovery rate, \\(\\mu\\), at a fixed value. Task Explore how the final size distribution varies with R0 by producing histograms of the final size distribution as per the above figure for values of \\(R_{0} = 20.0\\), \\(5.0\\), \\(2.0\\), \\(1.2\\), \\(1.0\\), \\(0.9\\), \\(0.5\\). Can you always identify the two modes of the distribution? 7.10 SIR Extension: Stochastic extinction One of the primary differences between the deterministic and stochastic formulations of the SIR model is that in the stochastic formulation the epidemic becomes extinct within a finite time, the so-called extinction time. The extinction time varies between realisations and in this section we are going to explore this distribution. 7.10.1 Question Which state variable do we need to consider in order to calculate the time to epidemic extinction and what is the value of this state variable at extinction? 7.10.2 Exercise one As with the final epidemic size, the extinction time will vary over realisations of the epidemic which will result in a distribution of extinction times, which we are now going to explore. Task Adapt your previous code to calculate the extinction time for each realisation. What value should the final time for the simulation be set to? Run 100 realisations in order to produce a histogram of the time to extinction, as per the figure below: Explore the distribution of extinction times for several values of \\(R_{0} = 20.0\\), \\(5.0\\), \\(2.0\\), \\(1.2\\), \\(1.0\\), \\(0.9\\), \\(0.5\\). What do you notice about the distribution as \\(R_{0}\\) gets close to 1? "],["estimating-r_0---p1.html", "8 Estimating \\(R_{0}\\) - P1 8.1 Introduction: Models and Data sets 8.2 Final Size Method 8.3 Regression Method 8.4 RECON earlyR method 8.5 Appendix", " 8 Estimating \\(R_{0}\\) - P1 Aim: To compare final size and exponential growth estimators for \\(R_{0}\\) using simulated data. Outline: Introduction: Models and Data sets Final Size Method Regression Method RECON earlyR Method 8.1 Introduction: Models and Data sets In Epidemic Practicals 1,2 &amp; 3 you wrote functions to numerically solve and simulate the deterministic and stochastic SIR models. For this practical we have extended these functions to add a latent class and implement the SEIR epidemic model introduced in lectures. If you have time at the end of this practical you can use these functions to simulate your own Outfluenza and Biggles and explore the performance of the final size and linear regression methods. However, so we all get the same results, we have provided simulated epidemics of Outfluenza and Biggles from which we will estimate \\(R_{0}\\) using the final size, (log)-linear regression and the earlyR method from the R Epidemics Consortium (RECON, https://www.repidemicsconsortium.org/). Copy Outfluenza1_cases.dat, Outfluenza2_cases.dat Biggles_cases.dat, SIRmodels.R and SEIRmodels.R which are provided in the shared folder to your working directory. The simulated outbreaks are provided as line lists, a common instrument used to collect data on individual cases during an outbreak where the date of infection (or rather the notification of a case) is recorded along with other relevant epidemiological information. For our purposes, the date of notification is sufficient to reconstruct the epidemic curve and test our different estimators of \\(R_{0}\\). Begin by reading in the line lists and converting the dates (stored as character strings) to date objects: require(chron) require(incidence) # Load line lists of cases for simulated outbreaks outfluenza1 &lt;- read.table(&#39;Outfluenza1_cases.dat&#39;) outfluenza1$x = as.Date(outfluenza1$x) outfluenza2 &lt;- read.table(&#39;Outfluenza2_cases.dat&#39;) outfluenza2$x = as.Date(outfluenza2$x) biggles &lt;- read.table(&#39;biggles_cases.dat&#39;) biggles$x = as.Date(biggles$x) We are going to use the incidence package from RECON which has been written to simplify computing, visualising and modelling incidence of infectious disease from dated event data such as line lists. To illustrate the key functions and concepts we will use incidence to construct daily incidence and cumulative incidence curves for the first outfluenza outbreak. The main workhorse function is called incidence which converts a list of dates into an incidence object that bins cases into a given interval: outfluenza1.i = incidence(outfluenza1$x,interval=1) We can see a summary of the incidence object by simply typing the name of the variable: outfluenza1.i ## &lt;incidence object&gt; ## [1183 cases from days 2019-02-24 to 2019-04-29] ## ## $counts: matrix with 65 rows and 1 columns ## $n: 1183 cases in total ## $dates: 65 dates marking the left-side of bins ## $interval: 1 day ## $timespan: 65 days ## $cumulative: FALSE So, the outfluenza1 outbreak consists of 1183 cases over the course of 65 days. We have chosen to bin cases on a daily interval (which is also the default so we could have simply left out this argument and will do so from now on). The incidence package provides custom plot functions for incidence objects: plot(outfluenza1.i) Which should be familiar from the lectures. You can try experimenting with different intervals to see how this affects the shape of the epidemic curve: plot(incidence(outfluenza1$x,interval=7)) incidence also allows you to subset curves between start and end dates: plot(subset(outfluenza1.i,from=outfluenza1$x[1],to=outfluenza1$x[100])) Task What is the significance of choosing the dates to subset in this way? Show: Solution Solution We have limited the graph to only plot the first 100 cases. We achieved this by using the date of the 100\\(^{th}\\) entry of the line list (outfluenza1$x) to set the upper limit. The cumulate() function returns a the cumulative incidence curve: plot(cumulate(outfluenza1.i)) Finally, you can convert an incidence object to a data frame by using the cast function data.frame(): outfluenza1.df = data.frame(outfluenza1.i) head(outfluenza1.df) ## dates counts ## 1 2019-02-24 1 ## 2 2019-02-25 0 ## 3 2019-02-26 0 ## 4 2019-02-27 0 ## 5 2019-02-28 1 ## 6 2019-03-01 2 8.2 Final Size Method Final size methods for \\(R_{0}\\) are in themselves a whole field of research. In this practical we will consider the utility of the basic deterministic first estimate also known as the final size formula: \\[\\begin{equation} \\tag{8.1} R_{0} = -\\frac{ln(1-z_{f})}{z_{f}} \\end{equation}\\] where \\(z_{f}\\) is the total fraction of the population infected at the end of an epidemic in a closed population. Task Write a function R0final that returns the value of \\(R_{0}\\) for a given value of \\(z_{f}\\). Is there any restriction on the values that \\(z_{f}\\) can take for the final size equation to be valid? Your function should check that \\(z_{f}\\) takes a valid value and return a missing value (NA) when the final size equation is not defined. Show: Solution Solution R0final &lt;- function(zf) { # zf must be proportion between 0 and 1 # Trap for division by zero(zf=0.0) # Trap for log(1) (-Inf) if(zf&lt;=0 || zf&gt;=1.0) { return(NA) } else { return(-log(1-zf)/zf) } } This formula is more commonly used to predict the final size (\\(z_{f}\\)) of an epidemic once we have an independent estimate of \\(R_{0}\\). This is less straightforward as we cannot solve equation (8.1) explicitly for \\(z_{f}\\). We can progress by rewriting (8.1) into a problem we can solve numerically: \\[\\begin{equation} \\tag{8.2} R_{0} + \\frac{ln(1-z_{f})}{z_{f}} = 0 \\end{equation}\\] We can find the final size (\\(z_{f}\\)) corresponding to a given value of \\(R_{0}\\), by solving for the value(s) of \\(z_{f}\\) for which (8.2) is equal to zero. Finding the roots of a function is a common problem, so it should not surprise you that there is a R function to do the job. uniroot uses an numerical procedure to estimate the roots of an arbitrary function f within a fixed interval=c(lower,upper). We can use uniroot to write a function that returns the predicted final size for a given value of \\(R_{0}\\): FinalSize &lt;- function(R0) { return(uniroot(function(zf) {R0+log(1-zf)/zf}, interval=c(.Machine$double.xmin,1.0))$root) } Check this function works by trying these test values on the R terminal: FinalSize(1.2) ## [1] 0.3136976 FinalSize(1.0) ## [1] 6.126547e-05 FinalSize(0.9) ## [1] -6.103516e-05 The exact values may differ slightly between versions of R on different machines. Note: Numerical methods are only accurate up to a specified precision or tolerance. This is fundamentally limited by the precision with which real numbers can be represented on a computer (that naturally works with integer or binary numbers). .Machine$double.xmin is the smallest number (or difference between two numbers) that can be stored in a given version of R. This value may change between versions and on different computer platforms (Windows, Mac OS, Linux) Task Use the auxiliary functions of the incidence package and these new functions you have just written to estimate \\(R_{0}\\) for the exemplar outfluenza and biggles outbreaks. (Remember the school size for all of these outbreaks was 1300 children.) Show: Solution Solution R0final(incidence(outfluenza1$x)$n/1300) ## [1] 2.646094 R0final(incidence(outfluenza2$x)$n/1300) ## [1] 2.378981 R0final(incidence(biggles$x)$n/1300) ## [1] 2.463143 8.3 Regression Method In the lectures we discussed how the early phase of an epidemic can be approximately modelled by an exponential growth model: \\[\\begin{equation} \\tag{8.3} I(t) = I(0)e^{\\Lambda t} \\end{equation}\\] where the exponential rate can be related to \\(R_{0}\\) with the functional form depending on the structure of the epidemic model (in particular with respect to the distribution of latent and infectious periods). If we take logs of both sides of equation (8.3) and rearrange we get the equation of a straight line: \\[\\begin{equation} \\tag{8.4} log(I(t)) = \\Lambda t + log(I(0)) \\end{equation}\\] with y-intercept given by the constant \\(log(I(0)\\) and slope \\(\\Lambda\\). So, we canp therefore obtain a first approximation to \\(R_{0}\\) simply by estimating the slope of the (logged) epidemic curve. In principle, estimating \\(R_{0}\\) using this method could be as straightforward as plotting the incidence or cumulative incidence curve on logarithmic graph paper and fitting a best-fit line, or using the Solver in Excel. We can be a little more sophisticated and use the linear regression model (lm) function in R. lm() uses a least squares method, effectively optimising the fit of the straight line to minimise the squared error between the line and the data. As the exponential approximation is only valid early in the epidemic, including data from the full epidemic curve would bias our estimate of \\(R_{0}\\). As discussed in the lecture we will use the first 100 cases. For a given simulation we need to select all the data-points up to the 100^{th} case. We can achieve this by sub setting the incidence object as before: outflu1.sub&lt;- data.frame(subset(outfluenza1.i,from=outfluenza1$x[1],to=outfluenza1$x[100])) We fit a linear model (best straight line fit) to find the slope of the log cumulative cases as described in the lectures: # Fit straight line to plot of biggles$t (time) and log of the cumulative cases (C) outflu1.fit = lm(log(1+counts) ~ dates, data=outflu1.sub) Note: We add 1 to counts to handle any zero cases (this should not affect the estimated slope). We can see the result of the regression by using the summary function: summary(outflu1.fit) ## ## Call: ## lm(formula = log(1 + counts) ~ dates, data = outflu1.sub) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7081 -0.3553 -0.1211 0.3675 0.9870 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.996e+03 7.855e+02 -7.633 1.77e-05 *** ## dates 3.340e-01 4.374e-02 7.635 1.77e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5231 on 10 degrees of freedom ## Multiple R-squared: 0.8536, Adjusted R-squared: 0.8389 ## F-statistic: 58.3 on 1 and 10 DF, p-value: 1.768e-05 Task The summary() function provides a lot of detailed information on the statistical fit of the regression line  for the purpose of estimating \\(R_{0}\\) the key value is the estimated slope highlighted in bold above. Using the expression for the SIR model presented in the lectures calculate the \\(R_{0}\\) of outfluenza using this slope: Show: Solution Solution \\(R_{0} = 1 + 0.335*5 = 2.67\\) We can estimate the uncertainty in our estimate of \\(R_{0}\\) from the uncertainty in our estimate of the slope from the linear regression. The R function confint() will calculate 95% confidence intervals for our regression model: confint(outflu1.fit) ## 2.5 % 97.5 % ## (Intercept) -7745.8850733 -4245.5519457 ## dates 0.2365213 0.4314554 Task Use these confidence intervals to calculate a 95% confidence interval for your estimate of \\(R_{0}\\). Show: Solution Solution \\(R_{0} = 2.67\\) (95% CI, 2.1 - 3.1) The incidence package has built in functions that simplify estimating the exponential growth rate. The base function is fit returns an incidence_fit object which returns details of the fit including the estimated exponential growth rate r and the confidence interval: outflu1.fit2 &lt;- fit(outfluenza1.i) outflu1.fit2 ## &lt;incidence_fit object&gt; ## ## $model: regression of log-incidence over time ## ## $info: list containing the following items: ## $r (daily growth rate): ## [1] -0.0504102 ## ## $r.conf (confidence interval): ## 2.5 % 97.5 % ## [1,] -0.06954703 -0.03127338 ## ## $halving (halving time in days): ## [1] 13.75014 ## ## $halving.conf (confidence interval): ## 2.5 % 97.5 % ## [1,] 9.966597 22.16413 ## ## $pred: data.frame of incidence predictions (57 rows, 5 columns) Task Why does fit ignore dates with 0 incidence? Show: Solution Solution We are fitting a log-linear relationship and \\(log(0)\\) is undefined. The estimated \\(r\\) is much lower than we obtained by our manual estimate. To see what has happened we can add the obtained fit to a plot of the incidence: plot(outfluenza1.i, fit= outflu1.fit2) We can use the subset function to limit our fit to the first 100 cases as before: outflu1.sub = subset(outfluenza1.i,from=outfluenza1$x[1],to=outfluenza1$x[100]) outflu1.fit3 = fit(outflu1.sub) plot(outflu1.sub,fit=outflu1.fit3) Task How do the estimates from the incidence package and our manual estimate compare to each other and why (whats different)? Show: Solution Solution outflu1.fit2 is clearly a poor fit to the exponential growth rate as we have not subsetted the data to the exponential phase. The point estimates from the incidence package and our manual estimate are pretty comparable, however the confidence intervals from our manual estimate are smaller (0.2-0.4 compared to 0.2-0.5) reflecting the effective smoothing we applied to the data by using the cumulative case counts instead of raw incidence. We could similarly smooth the time series by increasing the time interval to sum up incidence - this runs the risk of introducing artifacts to the data or giving us false confidence in our estimates. The incidence package provides a method for automatically sub setting the data by finding the optimum fit of two log-linear models (minimising the squared error of both): outflu1.fit5 = fit_optim_split(outfluenza1.i) plot(outfluenza1.i,fit=outflu1.fit5$fit) Note: The output of fit_optim_split now continues information on the two log-linear model fits, the first for the attack of the epidemic, the second describing the decay after the epidemic peak (which is also estimated as the changepoint between the two models). Examine the output by entering into the R console: outflu1.fit5 Which should also generate a plot of the mean r squared error (R2) of the two log-linear models against different dates for the changepoint: 8.4 RECON earlyR method Finally we will use the earlyR package implementation of the Tuenis and Wallinga method to estimate \\(R_{0}\\) and compare to the other methods. The key function of the earlyR package is the get_R function that returns an estimate of \\(R_{0}\\) when passed an incidence object and a serial interval distribution. The serial interval can be specified manually, but we will use the default gamma distribution which is parameterised by the mean (si_mean) and standard deviation (si_sd). For outfluenza, the serial interval distribution is exponentially distributed with mean equal to the variance (si_mean \\(= 5\\), si_sd \\(= 5\\)). library(earlyR) estR = get_R(outfluenza1.i,si_mean=5,si_sd=5) estR ## ## /// Early estimate of reproduction number (R) // ## // class: earlyR, list ## ## // Maximum-Likelihood estimate of R ($R_ml): ## [1] 1.001001 ## ## ## // $lambda: ## NA 0.1812692 0.1484107 0.1215084 0.09948267 0.2627188... ## ## // $dates: ## [1] &quot;2019-02-24&quot; &quot;2019-02-25&quot; &quot;2019-02-26&quot; &quot;2019-02-27&quot; &quot;2019-02-28&quot; ## [6] &quot;2019-03-01&quot; ## ... ## ## // $si (serial interval): ## A discrete distribution ## name: gamma ## parameters: ## shape: 1 ## scale: 5 Once again we have obtained a much lower estimate of \\(R (R_{0}=1.0)\\) than expected as the earlyR method is only valid during the exponential phase of the epidemic. If we subset again using the first 100 cases you should obtain a more consistent estimate of \\(R_{0}\\) of 2.43. We can calculated a boostrapped 95% confidence interval for the estimate using the sample_R function: quantile(sample_R(estR),c(0.025,0.975)) ## 2.5% 97.5% ## 0.960961 1.071071 So our earlyR estimate of \\(R_{0} = 2.43\\) (2.0-3.0, 95% CI). Task Now we have introduced all of the methods, lets compare the estimates (and confidence intervals where possible) of \\(R_{0}\\) for our three exemplar outbreaks. Complete the following table by adapted the code you have used in the practical. Remember that for Outfluenza the mean serial interval is 5 days (standard deviation 5 days). For Biggles the latent period is 5 days, the exposed period is 5 days. As discussed in lectures, adding two exponentially distributed variables together gives a gamma distributed variable. So the serial interval for Biggles will be gamma distributed with a scale parameter of 10 days and a shape parameter of 2, giving a mean serial interval of 10 and standard deviation of \\(5\\sqrt{2}\\). (Ask your instructor for more details if interested!) Outbreak Regression r Regression \\(R_{0}\\) Final Size \\(R_{0}\\) earlyR \\(R_{0}\\) Outfluenza1 Outfluenza2 Biggles Task Compare your estimates from the three methods of estimation. Are you surprised by the variability in estimates of \\(R_{0}\\) that you have obtained compared to the size of the 95% confidence intervals of the estimates? Show: Solution Solution The confidence intervals from our regression model capture the variability (scatter) in the model output around the best-fit line for this particular realisation of the epidemic model. Stochastic effects will generate far greater variability between realisations, which our linear model contains no information about. The uncertainty in the estimate of the slope will therefore typically underestimate the true variability in the epidemic model. The Likelihood function for the earlyR method includes more information on the variability of the serial interval derived from an epidemic model and better represents the true uncertainty in our estimates. 8.5 Appendix You might be interested now in explore how well the final size and regression estimators perform on different stochastic replicates of the SIR or SEIR models. SIRmodels.R and SEIRmodels.R provide functions to allow you to generate your own simulated epidemics of Biggles and Outfluenza. To load the functions into the R workspace using the following R code: source(&#39;SIRmodels.R&#39;) source(&#39;SEIRmodels.R&#39;) N = 1300 SIRmodels.R provides two familiar functions: DetSIR.dyn Uses the internal R function lsoda() to numerically solve the deterministic SIR model given 5 parameters: N: Population size I0: Initial infected B: transmission rate M: recovery rate = \\(1/T_{I}\\) corresponding to average infectious period of \\(T_{I}\\) f_time: run time for numerical solution create.StocSIR defines an stochastic SIR model to use with the SimInf package and takes 6 arguments: N: Population size I0: Initial infected beta: transmission rate m: recovery rate = \\(1/T_{I}\\) corresponding to average infectious period of \\(T_{I}\\) f_time: run time for numerical solution reps: number of replicates to simulate SEIRmodels.R provides equivalent functions DetSEIR.dyn and StocSEIR.dyn that implement the deterministic and stochastic SEIR epidemic model respectively. Both of these functions implementing the SEIR model take 7 parameters in the order that follows: N: Population size I0: Initial infected E0: Initial exposed B: transmission rate G: progression rate = \\(1/T_{E}\\), with average exposed period of \\(T_{E}\\) M: recovery rate = \\(1/T_{I}\\) corresponding to average infectious period of \\(T_{I}\\) f_time: run time for numerical solution The deterministic equations for the SEIR are then: \\[\\begin{aligned} \\frac{dS}{dt} &amp;= -\\frac{\\beta SI}{N} \\\\ \\frac{dE}{dt} &amp;= \\frac{\\beta SI}{N} - gE \\\\ \\frac{dI}{dt} &amp;= gE -mI \\\\ \\frac{dR}{dt} &amp;= mI \\end{aligned}\\] You can check this new code for the deterministic SEIR model by simulating an epidemic using the Biggles parameters from the lecture and comparing to the figures in the lecture notes: # Biggles Examplar Epidemic det.sol&lt;-DetSEIR.dyn(1300, 1,0, 0.5, 1/5.0, 1.0/5.0, 150) matplot(det.sol[,1],det.sol[,2:5],type=&#39;l&#39;,xlab=&#39;Days&#39;,ylab=&#39;Population&#39;,lwd=2) Checking the stochastic model is more difficult as the output will be different every time we run it! As a first sanity check we can compare stochastic simulations to the deterministic epidemic, checking that they (roughly) scatter evenly around the mean behaviour: SEIRmodel &lt;- create.StocSEIR(1300, 1, 0, 0.5, 1.0/5.0, 1/5.0, 150,1) out &lt;- run(model=SEIRmodel) plot(out) To simplify working with the incidence package we also provide a wrapper function line_list() that takes a SimInf object (out) as argument, along with a numeric value (node) and returns a line list of dates of cases for specified node: SEIRmodel &lt;- create.StocSEIR(1300, 1, 0, 0.5, 1.0/5.0, 1/5.0, 150,100) out &lt;- run(model=SEIRmodel) line_list(out,1) "],["seasonality-and-measles-epidemics.html", "9 Seasonality and Measles Epidemics 9.1 Measles data and challenge 9.2 Standard SEIR (exponential) with sinusoidal forcing 9.3 Standard SEIR (exponential) with term-time forcing 9.4 Gamma SEIR with term-time forcing", " 9 Seasonality and Measles Epidemics 9.1 Measles data and challenge You have been asked to implement a determinsitic model to predict the impact of vaccination on the timing of outbreaks of measles in London. You have been asked to make a number of assumptions to proceed: Constant population size of 3.3 million Birth rate of 20 per thousand per year Basic reproduction number \\(R_{0} = 17\\) (for sinusoidal forcing model) Cases can be approximately calculated from the number of infectives by multiplying by 7/5 (i.e. reporting period /average infectious period) You were provided with a time-series of measles cases from London from 1950-1964 (immediately prior to the introduction of vaccination in the UK) and asked to assume that the reporting rate at this time was 40%. These data are included as part of the tsiR package in R that provides historical time-series data from England and Wales along with functions to work with the so-called time-series SIR model (TSIR) - a discrete time chain binomial model that can be used to very successfully model and predict measles dynamics (and to a lesser extent other strongly immunizing childhood infections). The plotdata function from the tsiR package provides a summary of the incidence and demographic data (birth rates and population size): As you can see, our simplifying assumption of fixed birth and population size glosses over the baby boom post WWII that pushed the typical two-year cycle of measles epidemics into annual outbreaks and led (along with migration) to an increase in the population size. The TSIR model can be used to estimate the seasonal variation in transmission rates presented in the background slides (on which we overlay the typical pattern of school terms in England and Wales calculated by the mk_terms function below): ## alpha mean beta mean rho mean sus ## 9.60e-01 1.19e-05 4.57e-01 1.14e+05 ## prop. init. sus. prop. init. inf. ## 3.01e-02 6.12e-05 Important to note that the estimated transmission parameters from the TSIR model do not translate directly to the transmission parameters for continuous-time (ordinary differential or stochastic) models. The qualitative pattern is informative rather than the specific estimates. (Conceptually I would consider these estimates to be closer to reproduction numbers than transmission rates as they are usually presented.). Note alsoe that the \\(\\alpha\\) used in the TSIR model is not an amplitude of seasonality but a correction (i.e. fudge) factor for the density dependence of the transmission term which - in some sense - can be used to adjust for artificts arising from the discrete time approximation. We also provided you with hope-simpons estimates of the serial interval of measles and illustrated that it suggests that the latent and infectious periods of measles are less dispersed (less variable) than exponential and well described by a gamma distribution where the shape and scale parameters are approximately equal: 9.2 Standard SEIR (exponential) with sinusoidal forcing We first present an implementation of the standard SEIR model - withc constant rates of progression through the latent and infectious compartments - and sinusoidal forcing term. The basic template here is similar to what you have seen before in earlier practicals except that the transmission rate now depends on the time. Here we wrote a wrapper function which sets some sensible default initial conditions (the fixed points of the unforced SEIR model where \\(\\alpha=0.0\\)) and then runs the model for three periods. No matter the inital model conditions the seasonally forced SEIR model will eventually converge towards a stable dynamic behaviour (often described as an attractor in analogy to the fixed points of an unforced model). We can run the model for a burn-in period (analogous to the same concept for MCMC convergence) to remove this transient behaviour. Our function solves the model for three periods. We forward-simulate the initial conditions for burnin years discarding the results but using the final state to set the initial conditions for a run of prevacc years. We then adjust the birth rate to model vaccination at birth and then run for an additional postvacc years, returning a tibble with the sampling times and state variables of the model: Using the suggested parameter values you should find that a seasonal forcing of \\(\\alpha = 0.19\\) gives a reasonable qualitative fit to the two year cycle of measles oubtreaks seen in London after 1950: For sinusoidal forcing the bifurcation to two-yearly cycles occurs for a very small amount of seasonal forcing (\\(\\alpha \\sim 0.02\\)) with the amplitude of the two-year cycle increasing with increasing \\(\\alpha\\) before further bifurcations to three, four and irregular multiannual cycles. If we now examine the effect of increasing vaccination coverage we see that at low coverage vaccination progressively increases the time between outbreaks - shifting cycles to 3,4 and higher (irregular cycles). However, the incidence between outbreaks now falls to unrealistically low levels (nano-scale number of infectives!) - highlighting the likely increased importance of stochastic effects after the introduction of vaccination. Note: These types of models can be very sensitive to both initial conditions and numerical errors meaning that more sophisticated numerical methods (such as AUTO) need to be used to map out the full range of dynamical behaviours accurately. 9.3 Standard SEIR (exponential) with term-time forcing We now adapt the previous code to use a term-time forcing function and run the same model scenarios: The term-time forcing function leads to a very different (and richer) bifurcation structure. Indeed, if you tried this form of the model you likely could not find values of \\(\\alpha\\) that gave a reasonable match to the London time-series for the assumed parameter values. Reducing the assumed value of \\(R_{0}\\) to 12 you can achieve a comparable fit (arguably better with respect to the qualitative shape of the attractor). We see the same qualiative impact of vaccination as before, but note that the higher troughs in prevalence between major outbreaks predicted by the term-time forcing model mean that stable (and plausible in terms of depth of trough between outbreaks) dynamics are seen for a wider range of birth/vaccination rates: 9.4 Gamma SEIR with term-time forcing Finally, we present an implementation of the term-time forcing model with realistic (gamma) distributed latent and infectious periods. The Gamma-SEIR model is much more sensitive to changes in the forcing amplitude \\(\\alpha\\) requiring lower amplitudes to generate the same qualitative dynamics as the (less biologically accurate) standard model with constant rates. These models are even more difficult to work with numerically and presented here purely for completeness and a cautionary note on the extent to which these model assumptions impact on the range of model parameters that will be consistent with real data. This sensitivity - and strong tradeoffs between transmission rates, birth rates and distributional assumptions makes this type of model particular problematic (and therefore particulary interesting) to perform inference with. "],["outbreak-of-influenza-in-a-boarding-school.html", "10 Outbreak of influenza in a boarding school 10.1 Data Summary and challenge 10.2 Deterministic model 10.3 Stochastic model", " 10 Outbreak of influenza in a boarding school 10.1 Data Summary and challenge We will build and roughly calibrate a transmission model to describe the outbreak of influenza in a English boarding school we first met in the stochastic modelling practical. For the purposes of this practical we assume that the numbers of pupils in bed measure the numbers infectious on each date. Not a great assumption as they are to an extent self-isolating, but also not terrible given the relatively short latent and (effective) infectious period for influenza (\\(\\sim 1\\) day). We are asked to model the likely impact on the outbreak should 80% of the boys have been vaccinated before the start of the outbreak with a vaccine with 50% (direct) protection from infection. For a single outbreak reasonable to neglect the potential for loss of immunity to reinfection so modelling vaccination in this case only amounts to changing the initial conditions of the model (i.e. the number susceptible and recovered when infection is introduced.) We are told to assume an exposed period of 1 hour and an infectious period of 2 days so a standard SEIR compartmental model should be sufficient. We will first address the question using a deterministic and stochastic model in turn. 10.2 Deterministic model For the standard SEIR model with constant rates of progression through the exposed and infectious compartments an \\(R_0\\) of \\(\\sim 4.0\\) gives a rough calibration to the observed data (black points and lines): This rough calibration does not hold up if we now vary the distributional assumption for the latent and infectious periods. The supplied code has implemented the SEIR model with gamma distributed (strictly an erlang distribution were the shape parameter is an integer) latent and infectious periods. If we vary the shape parameters for I and E we see that the peak prevalence and initial rate of exponential growth are very sensitive to changes in the distribution. Less dispersed distributions (i.e. higher shape parameters) have sharper epidemics with higher peak prevalence. The final size (and approximate value of R0) on the other had are insensitive to these changes. Below we plot a set of three curves below for shape parameters = 10 illustrating that we now require a lower value of \\(R_{0}\\) to (roughly) match the data. The fit is not as good as I deliberately picked the average infectious and latent periods to give a reasonable fit with the exponential model In practice the latent and infectious period distributions are not identifiable purely from a single epidemic curve. As this very simple comparison demonstrates calibrating our model based on the wrong assumption can lead to a significant under or over estimate of \\(R_{0}\\). 10.3 Stochastic model The rough calibration we identified using the deterministic model mostly holds up for the stochastic model with replicates scattering around the observed numbers. Although the population is small, the transmission rate is relatively high (\\(R_{0}=4\\)) so the deterministic model is a reasonable approximation for the average of the stochastic simulations. If the transmission rate was lower this would not necessarily be the case. The main reason for considering a stochastic model for this question is then to quantify how vaccination changes the chances of seeing an outbreak at all. We can calculate this from our simulated scenarios by calculate the mean number of simulations that have more than one infection ( mean(outbreak) in table below) or compare the final size distribution for the two scenarios. ## # A tibble: 2 x 2 ## vacc `mean(outbreak)` ## &lt;lgl&gt; &lt;dbl&gt; ## 1 FALSE 0.75 ## 2 TRUE 0.36 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
